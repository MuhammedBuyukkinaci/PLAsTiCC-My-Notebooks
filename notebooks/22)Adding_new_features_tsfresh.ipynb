{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import gc; gc.enable()\n",
    "from functools import partial, wraps\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "np.random.seed(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def haversine_plus(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees) from\n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    #Convert decimal degrees to Radians:\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    #Implementing Haversine Formula:\n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),\n",
    "                          np.multiply(np.cos(lat1),\n",
    "                                      np.multiply(np.cos(lat2),\n",
    "                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n",
    "\n",
    "    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n",
    "    return {\n",
    "        'haversine': haversine,\n",
    "        'latlon1': np.subtract(np.multiply(lon1, lat1), np.multiply(lon2, lat2)),\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def process_flux(df):\n",
    "    flux_ratio_sq = np.power(df['flux'].values / df['flux_err'].values, 2.0)\n",
    "\n",
    "    df_flux = pd.DataFrame({\n",
    "        'flux_ratio_sq': flux_ratio_sq,\n",
    "        'flux_by_flux_ratio_sq': df['flux'].values * flux_ratio_sq,},\n",
    "        index=df.index)\n",
    "\n",
    "    return pd.concat([df, df_flux], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def process_flux_agg(df):\n",
    "    flux_w_mean = df['flux_by_flux_ratio_sq_sum'].values / df['flux_ratio_sq_sum'].values\n",
    "    flux_diff = df['flux_max'].values - df['flux_min'].values\n",
    "\n",
    "    df_flux_agg = pd.DataFrame({\n",
    "        'flux_w_mean': flux_w_mean,\n",
    "        'flux_diff1': flux_diff,\n",
    "        'flux_diff2': flux_diff / df['flux_mean'].values,\n",
    "        'flux_diff3': flux_diff /flux_w_mean,\n",
    "        }, index=df.index)\n",
    "\n",
    "    return pd.concat([df, df_flux_agg], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(df, df_meta, aggs, fcp, n_jobs=8):\n",
    "    \"\"\"\n",
    "    Extracting Features from train set\n",
    "    Features from olivier's kernel\n",
    "    very smart and powerful feature that is generously given here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    per passband features with tsfresh library. fft features added to capture periodicity https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70346#415506\n",
    "    \"\"\"\n",
    "\n",
    "    df = process_flux(df)\n",
    "\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    agg_df.columns = [ '{}_{}'.format(k, agg) for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df = process_flux_agg(agg_df) # new feature to play with tsfresh\n",
    "\n",
    "    # Add more features with\n",
    "    agg_df_ts_flux_passband = extract_features(df,\n",
    "                                               column_id='object_id',\n",
    "                                               column_sort='mjd',\n",
    "                                               column_kind='passband',\n",
    "                                               column_value='flux',\n",
    "                                               default_fc_parameters=fcp['flux_passband'], n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux = extract_features(df,\n",
    "                                      column_id='object_id',\n",
    "                                      column_value='flux',\n",
    "                                      default_fc_parameters=fcp['flux'], n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux_by_flux_ratio_sq = extract_features(df,\n",
    "                                      column_id='object_id',\n",
    "                                      column_value='flux_by_flux_ratio_sq',\n",
    "                                      default_fc_parameters=fcp['flux_by_flux_ratio_sq'], n_jobs=n_jobs)\n",
    "\n",
    "    # Add smart feature that is suggested here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    # dt[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n",
    "    df_det = df[df['detected']==1].copy()\n",
    "    agg_df_mjd = extract_features(df_det,\n",
    "                                  column_id='object_id',\n",
    "                                  column_value='mjd',\n",
    "                                  default_fc_parameters=fcp['mjd'], n_jobs=n_jobs)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'].values - agg_df_mjd['mjd__minimum'].values\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "\n",
    "    agg_df_ts_flux_passband.index.rename('object_id', inplace=True)\n",
    "    agg_df_ts_flux.index.rename('object_id', inplace=True)\n",
    "    agg_df_ts_flux_by_flux_ratio_sq.index.rename('object_id', inplace=True)\n",
    "    agg_df_mjd.index.rename('object_id', inplace=True)\n",
    "    agg_df_ts = pd.concat([agg_df,\n",
    "                           agg_df_ts_flux_passband,\n",
    "                           agg_df_ts_flux,\n",
    "                           agg_df_ts_flux_by_flux_ratio_sq,\n",
    "                           agg_df_mjd], axis=1).reset_index()\n",
    "\n",
    "    result = agg_df_ts.merge(right=df_meta, how='left', on='object_id')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_meta(filename):\n",
    "    meta_df = pd.read_csv(filename)\n",
    "\n",
    "    meta_dict = dict()\n",
    "    # distance\n",
    "    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values,\n",
    "                   meta_df['gal_l'].values, meta_df['gal_b'].values))\n",
    "    #\n",
    "    meta_dict['hostgal_photoz_certain'] = np.multiply(\n",
    "            meta_df['hostgal_photoz'].values,\n",
    "             np.exp(meta_df['hostgal_photoz_err'].values))\n",
    "\n",
    "    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_true, y_preds, classes, class_weights):\n",
    "    \"\"\"\n",
    "    refactor from\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weights[k] for k in sorted(class_weights.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    refactor from\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "\n",
    "    loss = multi_weighted_logloss(y_true, y_preds, classes, class_weights)\n",
    "    return 'wloss', loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_multi_weighted_logloss(y_predicted, y_true, classes, class_weights):\n",
    "    loss = multi_weighted_logloss(y_true.get_label(), y_predicted,\n",
    "                                  classes, class_weights)\n",
    "    return 'wloss', loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    return importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_modeling_cross_validation(params,\n",
    "                                  full_train,\n",
    "                                  y,\n",
    "                                  classes,\n",
    "                                  class_weights,\n",
    "                                  nr_fold=5,\n",
    "                                  random_state=51):\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "    # loss function\n",
    "    func_loss = partial(xgb_multi_weighted_logloss,\n",
    "                        classes=classes,\n",
    "                        class_weights=class_weights)\n",
    "\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold,\n",
    "                            shuffle=True,\n",
    "                            random_state=random_state)\n",
    "\n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=func_loss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, ntree_limit=clf.best_ntree_limit)\n",
    "        print('no {}-fold loss: {}'.format(fold_ + 1,\n",
    "              multi_weighted_logloss(val_y, oof_preds[val_, :],\n",
    "                                     classes, class_weights)))\n",
    "\n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds,\n",
    "                                   classes=classes, class_weights=class_weights)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    df_importances = save_importances(importances_=importances)\n",
    "    df_importances.to_csv('xgb_importances.csv', index=False)\n",
    "\n",
    "    return clfs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_modeling_cross_validation(params,\n",
    "                                   full_train,\n",
    "                                   y,\n",
    "                                   classes,\n",
    "                                   class_weights,\n",
    "                                   nr_fold=5,\n",
    "                                   random_state=51):\n",
    "\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold,\n",
    "                            shuffle=True,\n",
    "                            random_state=random_state)\n",
    "\n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = LGBMClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgbm_multi_weighted_logloss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        print('no {}-fold loss: {}'.format(fold_ + 1,\n",
    "              multi_weighted_logloss(val_y, oof_preds[val_, :],\n",
    "                                     classes, class_weights)))\n",
    "\n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds,\n",
    "                                   classes=classes, class_weights=class_weights)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    df_importances = save_importances(importances_=importances)\n",
    "    df_importances.to_csv('lgbm_importances.csv', index=False)\n",
    "\n",
    "    return clfs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chunk(df_, clfs_, meta_, features, featurize_configs, train_mean):\n",
    "\n",
    "    # process all features\n",
    "    full_test = featurize(df_, meta_,\n",
    "                          featurize_configs['aggs'],\n",
    "                          featurize_configs['fcp'])\n",
    "    full_test.fillna(0, inplace=True)\n",
    "\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test[features])\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test[features])\n",
    "\n",
    "    preds_ = preds_ / len(clfs_)\n",
    "\n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_,\n",
    "                             columns=['class_{}'.format(s) for s in clfs_[0].classes_])\n",
    "    preds_df_['object_id'] = full_test['object_id']\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99)\n",
    "    #print(full_test[['object_id'] + features].shape)\n",
    "    print(list(full_test.columns.values))\n",
    "    return full_test[['object_id'] + list(features)],preds_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test(clfs,\n",
    "                 features,\n",
    "                 featurize_configs,\n",
    "                 train_mean,\n",
    "                 filename='predictions2.csv',\n",
    "                 chunks=5000000):\n",
    "    start = time.time()\n",
    "\n",
    "    meta_test = process_meta('test_set_metadata.csv')\n",
    "    # meta_test.set_index('object_id',inplace=True)\n",
    "\n",
    "    remain_df = None\n",
    "    for i_c, df in enumerate(pd.read_csv('test_set.csv', chunksize=chunks, iterator=True)):\n",
    "        # Check object_ids\n",
    "        # I believe np.unique keeps the order of group_ids as they appear in the file\n",
    "        unique_ids = np.unique(df['object_id'])\n",
    "\n",
    "        new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
    "        if remain_df is None:\n",
    "            df = df.loc[df['object_id'].isin(unique_ids[:-1])]\n",
    "        else:\n",
    "            df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
    "        # Create remaining samples df\n",
    "        remain_df = new_remain_df\n",
    "\n",
    "        full_test2,preds_df = predict_chunk(df_=df,\n",
    "                                 clfs_=clfs,\n",
    "                                 meta_=meta_test,\n",
    "                                 features=features,\n",
    "                                 featurize_configs=featurize_configs,\n",
    "                                 train_mean=train_mean)\n",
    "\n",
    "        if i_c == 0:\n",
    "            preds_df.to_csv(filename, header=True, mode='a', index=False)\n",
    "        else:\n",
    "            preds_df.to_csv(filename, header=False, mode='a', index=False)\n",
    "        if i_c == 0:\n",
    "            full_test2.to_csv('klm_test.csv', header=True, mode='a', index=False)\n",
    "        else:\n",
    "            full_test2.to_csv('klm_test.csv', header=False, mode='a', index=False)\n",
    "\n",
    "        del preds_df,full_test2\n",
    "        gc.collect()\n",
    "        print('{:15d} done in {:5.1f} minutes' .format(\n",
    "                chunks * (i_c + 1), (time.time() - start) / 60), flush=True)\n",
    "\n",
    "    # Compute last object in remain_df\n",
    "    full_test2,preds_df = predict_chunk(df_=remain_df,\n",
    "                             clfs_=clfs,\n",
    "                             meta_=meta_test,\n",
    "                             features=features,\n",
    "                             featurize_configs=featurize_configs,\n",
    "                             train_mean=train_mean)\n",
    "\n",
    "    print(\"no problem\")\n",
    "    print(preds_df.shape,full_test2.shape)\n",
    "    full_test2.to_csv('klm_test.csv', header=False, mode='a', index=False)\n",
    "    preds_df.to_csv(filename, header=False, mode='a', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = {\n",
    "    'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "    'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "    'detected': ['mean'],\n",
    "    'flux_ratio_sq':['sum', 'skew'],\n",
    "    'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "}\n",
    "\n",
    "# tsfresh features\n",
    "fcp = {\n",
    "    'flux': {\n",
    "        'longest_strike_above_mean': None,\n",
    "        'longest_strike_below_mean': None,\n",
    "        'mean_change': None,\n",
    "        'mean_abs_change': None,\n",
    "        'length': None,\n",
    "    },\n",
    "\n",
    "    'flux_by_flux_ratio_sq': {\n",
    "        'longest_strike_above_mean': None,\n",
    "        'longest_strike_below_mean': None,\n",
    "    },\n",
    "\n",
    "    'flux_passband': {\n",
    "        'fft_coefficient': [\n",
    "                {'coeff': 0, 'attr': 'abs'},\n",
    "                {'coeff': 1, 'attr': 'abs'}\n",
    "            ],\n",
    "        'kurtosis' : None,\n",
    "        'skewness' : None,\n",
    "    },\n",
    "\n",
    "    'mjd': {\n",
    "        'maximum': None,\n",
    "        'minimum': None,\n",
    "        'mean_change': None,\n",
    "        'mean_abs_change': None,\n",
    "    },\n",
    "}\n",
    "\n",
    "best_params = {\n",
    "        'device': 'cpu',\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 14,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_jobs': -1,\n",
    "        'max_depth': 7,\n",
    "        'n_estimators': 500,\n",
    "        'subsample_freq': 2,\n",
    "        'subsample_for_bin': 5000,\n",
    "        'min_data_per_group': 100,\n",
    "        'max_cat_to_onehot': 4,\n",
    "        'cat_l2': 1.0,\n",
    "        'cat_smooth': 59.5,\n",
    "        'max_cat_threshold': 32,\n",
    "        'metric_freq': 10,\n",
    "        'verbosity': -1,\n",
    "        'metric': 'multi_logloss',\n",
    "        'xgboost_dart_mode': False,\n",
    "        'uniform_drop': False,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'drop_rate': 0.173,\n",
    "        'learning_rate': 0.0267,\n",
    "        'max_drop': 5,\n",
    "        'min_child_samples': 10,\n",
    "        'min_child_weight': 100.0,\n",
    "        'min_split_gain': 0.1,\n",
    "        'num_leaves': 7,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.00023,\n",
    "        'skip_drop': 0.44,\n",
    "        'subsample': 0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 40/40 [00:06<00:00,  7.58it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:01<00:00, 30.17it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:00<00:00, 45.74it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:00<00:00, 70.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.8 s, sys: 1.09 s, total: 22.8 s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meta_train = process_meta('training_set_metadata.csv')\n",
    "\n",
    "train = pd.read_csv('training_set.csv')\n",
    "full_train = featurize(train, meta_train, aggs, fcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'target' in full_train:\n",
    "    y = full_train['target']\n",
    "    del full_train['target']\n",
    "\n",
    "classes = sorted(y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes : 14, [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
      "{6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n"
     ]
    }
   ],
   "source": [
    "# Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "# with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "class_weights = {c: 1 for c in classes}\n",
    "class_weights.update({c:2 for c in [64, 15]})\n",
    "print('Unique classes : {}, {}'.format(len(classes), classes))\n",
    "print(class_weights)\n",
    "#sanity check: classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "#sanity check: class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "#if len(np.unique(y_true)) > 14:\n",
    "#    classes.append(99)\n",
    "#    class_weights[99] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'object_id' in full_train:\n",
    "    oof_df = full_train[['object_id']]\n",
    "    del full_train['object_id']\n",
    "    #del full_train['distmod']\n",
    "    del full_train['hostgal_specz']\n",
    "    del full_train['ra'], full_train['decl'], full_train['gal_l'], full_train['gal_b']\n",
    "    del full_train['ddf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 ms, sys: 88 µs, total: 2.18 ms\n",
      "Wall time: 1.81 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "klm = pd.concat([oof_df,full_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>flux_min</th>\n",
       "      <th>flux_max</th>\n",
       "      <th>flux_mean</th>\n",
       "      <th>flux_median</th>\n",
       "      <th>flux_std</th>\n",
       "      <th>flux_skew</th>\n",
       "      <th>flux_err_min</th>\n",
       "      <th>flux_err_max</th>\n",
       "      <th>flux_err_mean</th>\n",
       "      <th>flux_err_median</th>\n",
       "      <th>flux_err_std</th>\n",
       "      <th>flux_err_skew</th>\n",
       "      <th>detected_mean</th>\n",
       "      <th>flux_ratio_sq_sum</th>\n",
       "      <th>flux_ratio_sq_skew</th>\n",
       "      <th>flux_by_flux_ratio_sq_sum</th>\n",
       "      <th>flux_by_flux_ratio_sq_skew</th>\n",
       "      <th>flux_w_mean</th>\n",
       "      <th>flux_diff1</th>\n",
       "      <th>flux_diff2</th>\n",
       "      <th>flux_diff3</th>\n",
       "      <th>0__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>0__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>0__kurtosis</th>\n",
       "      <th>0__skewness</th>\n",
       "      <th>1__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>1__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>1__kurtosis</th>\n",
       "      <th>1__skewness</th>\n",
       "      <th>2__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>2__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>2__kurtosis</th>\n",
       "      <th>2__skewness</th>\n",
       "      <th>3__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>3__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>3__kurtosis</th>\n",
       "      <th>3__skewness</th>\n",
       "      <th>4__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>4__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>4__kurtosis</th>\n",
       "      <th>4__skewness</th>\n",
       "      <th>5__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>5__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>5__kurtosis</th>\n",
       "      <th>5__skewness</th>\n",
       "      <th>flux__length</th>\n",
       "      <th>flux__longest_strike_above_mean</th>\n",
       "      <th>flux__longest_strike_below_mean</th>\n",
       "      <th>flux__mean_abs_change</th>\n",
       "      <th>flux__mean_change</th>\n",
       "      <th>flux_by_flux_ratio_sq__longest_strike_above_mean</th>\n",
       "      <th>flux_by_flux_ratio_sq__longest_strike_below_mean</th>\n",
       "      <th>mjd__mean_abs_change</th>\n",
       "      <th>mjd__mean_change</th>\n",
       "      <th>mjd_diff_det</th>\n",
       "      <th>hostgal_photoz</th>\n",
       "      <th>hostgal_photoz_err</th>\n",
       "      <th>distmod</th>\n",
       "      <th>mwebv</th>\n",
       "      <th>target</th>\n",
       "      <th>haversine</th>\n",
       "      <th>latlon1</th>\n",
       "      <th>hostgal_photoz_certain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>-1100.440063</td>\n",
       "      <td>660.626343</td>\n",
       "      <td>-123.096998</td>\n",
       "      <td>-89.477524</td>\n",
       "      <td>394.109851</td>\n",
       "      <td>-0.349540</td>\n",
       "      <td>2.130510</td>\n",
       "      <td>12.845472</td>\n",
       "      <td>4.482743</td>\n",
       "      <td>3.835269</td>\n",
       "      <td>1.744747</td>\n",
       "      <td>1.623740</td>\n",
       "      <td>0.946023</td>\n",
       "      <td>2.929669e+06</td>\n",
       "      <td>0.812722</td>\n",
       "      <td>-9.601766e+08</td>\n",
       "      <td>-1.414322</td>\n",
       "      <td>-327.742307</td>\n",
       "      <td>1761.066406</td>\n",
       "      <td>-14.306331</td>\n",
       "      <td>-5.373326</td>\n",
       "      <td>205.036926</td>\n",
       "      <td>1628.427737</td>\n",
       "      <td>-1.475181</td>\n",
       "      <td>0.128917</td>\n",
       "      <td>22370.594834</td>\n",
       "      <td>2806.374162</td>\n",
       "      <td>-1.255123</td>\n",
       "      <td>0.415580</td>\n",
       "      <td>7780.500807</td>\n",
       "      <td>2805.598113</td>\n",
       "      <td>-1.409885</td>\n",
       "      <td>0.339918</td>\n",
       "      <td>7024.003068</td>\n",
       "      <td>2536.068846</td>\n",
       "      <td>-1.449858</td>\n",
       "      <td>0.293128</td>\n",
       "      <td>3245.366349</td>\n",
       "      <td>2741.539785</td>\n",
       "      <td>-1.548319</td>\n",
       "      <td>0.200096</td>\n",
       "      <td>2704.641265</td>\n",
       "      <td>2893.344217</td>\n",
       "      <td>-1.592820</td>\n",
       "      <td>0.125268</td>\n",
       "      <td>352.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>202.114067</td>\n",
       "      <td>1.999688</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.631898</td>\n",
       "      <td>2.631898</td>\n",
       "      <td>873.7903</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>92</td>\n",
       "      <td>0.319006</td>\n",
       "      <td>-1.528827</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>713</td>\n",
       "      <td>-14.735178</td>\n",
       "      <td>14.770886</td>\n",
       "      <td>-1.423351</td>\n",
       "      <td>-0.873033</td>\n",
       "      <td>6.471144</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.639458</td>\n",
       "      <td>9.115748</td>\n",
       "      <td>2.359620</td>\n",
       "      <td>1.998217</td>\n",
       "      <td>1.509888</td>\n",
       "      <td>1.633246</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>5.886068e+03</td>\n",
       "      <td>3.439423</td>\n",
       "      <td>-2.875087e+04</td>\n",
       "      <td>-3.454554</td>\n",
       "      <td>-4.884564</td>\n",
       "      <td>29.506064</td>\n",
       "      <td>-20.730002</td>\n",
       "      <td>-6.040676</td>\n",
       "      <td>190.427851</td>\n",
       "      <td>299.586559</td>\n",
       "      <td>-1.014003</td>\n",
       "      <td>0.260052</td>\n",
       "      <td>57.109047</td>\n",
       "      <td>192.539229</td>\n",
       "      <td>-1.097170</td>\n",
       "      <td>-0.087865</td>\n",
       "      <td>44.477327</td>\n",
       "      <td>191.057528</td>\n",
       "      <td>-1.188472</td>\n",
       "      <td>-0.022678</td>\n",
       "      <td>55.270113</td>\n",
       "      <td>212.522263</td>\n",
       "      <td>-1.142896</td>\n",
       "      <td>-0.167176</td>\n",
       "      <td>50.414646</td>\n",
       "      <td>203.892482</td>\n",
       "      <td>-1.190245</td>\n",
       "      <td>-0.064134</td>\n",
       "      <td>100.473776</td>\n",
       "      <td>143.963093</td>\n",
       "      <td>-0.797047</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>350.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.935177</td>\n",
       "      <td>-0.050944</td>\n",
       "      <td>199.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.352571</td>\n",
       "      <td>14.352571</td>\n",
       "      <td>846.8017</td>\n",
       "      <td>1.6267</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>45.4063</td>\n",
       "      <td>0.007</td>\n",
       "      <td>88</td>\n",
       "      <td>1.698939</td>\n",
       "      <td>3.258921</td>\n",
       "      <td>2.099614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>730</td>\n",
       "      <td>-19.159811</td>\n",
       "      <td>47.310059</td>\n",
       "      <td>2.267434</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>8.022239</td>\n",
       "      <td>3.177854</td>\n",
       "      <td>0.695106</td>\n",
       "      <td>11.281384</td>\n",
       "      <td>2.471061</td>\n",
       "      <td>1.990851</td>\n",
       "      <td>1.721134</td>\n",
       "      <td>1.823726</td>\n",
       "      <td>0.069697</td>\n",
       "      <td>4.124452e+03</td>\n",
       "      <td>5.480405</td>\n",
       "      <td>1.046502e+05</td>\n",
       "      <td>5.989138</td>\n",
       "      <td>25.373110</td>\n",
       "      <td>66.469870</td>\n",
       "      <td>29.315018</td>\n",
       "      <td>2.619697</td>\n",
       "      <td>3.461790</td>\n",
       "      <td>4.729538</td>\n",
       "      <td>0.474215</td>\n",
       "      <td>0.356910</td>\n",
       "      <td>7.334944</td>\n",
       "      <td>13.515895</td>\n",
       "      <td>0.976374</td>\n",
       "      <td>0.471342</td>\n",
       "      <td>124.845250</td>\n",
       "      <td>119.500254</td>\n",
       "      <td>5.131290</td>\n",
       "      <td>2.385066</td>\n",
       "      <td>168.280524</td>\n",
       "      <td>162.799417</td>\n",
       "      <td>7.125665</td>\n",
       "      <td>2.662075</td>\n",
       "      <td>219.745132</td>\n",
       "      <td>202.532898</td>\n",
       "      <td>6.081065</td>\n",
       "      <td>2.537802</td>\n",
       "      <td>231.509177</td>\n",
       "      <td>199.286370</td>\n",
       "      <td>3.583130</td>\n",
       "      <td>1.680352</td>\n",
       "      <td>330.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.227614</td>\n",
       "      <td>-0.008131</td>\n",
       "      <td>4.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>3.580623</td>\n",
       "      <td>3.580623</td>\n",
       "      <td>78.7737</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>40.2561</td>\n",
       "      <td>0.021</td>\n",
       "      <td>42</td>\n",
       "      <td>1.818030</td>\n",
       "      <td>3.128522</td>\n",
       "      <td>0.229779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>745</td>\n",
       "      <td>-15.494463</td>\n",
       "      <td>220.795212</td>\n",
       "      <td>8.909206</td>\n",
       "      <td>1.035895</td>\n",
       "      <td>27.558208</td>\n",
       "      <td>4.979826</td>\n",
       "      <td>0.567170</td>\n",
       "      <td>55.892746</td>\n",
       "      <td>2.555576</td>\n",
       "      <td>1.819875</td>\n",
       "      <td>3.537324</td>\n",
       "      <td>10.741655</td>\n",
       "      <td>0.173789</td>\n",
       "      <td>9.416165e+04</td>\n",
       "      <td>9.611274</td>\n",
       "      <td>1.439125e+07</td>\n",
       "      <td>11.141069</td>\n",
       "      <td>152.835617</td>\n",
       "      <td>236.289675</td>\n",
       "      <td>26.521968</td>\n",
       "      <td>1.546038</td>\n",
       "      <td>129.421659</td>\n",
       "      <td>123.298327</td>\n",
       "      <td>4.629801</td>\n",
       "      <td>2.023211</td>\n",
       "      <td>320.174052</td>\n",
       "      <td>280.440312</td>\n",
       "      <td>50.868880</td>\n",
       "      <td>7.007099</td>\n",
       "      <td>543.845781</td>\n",
       "      <td>491.548270</td>\n",
       "      <td>36.088137</td>\n",
       "      <td>5.688194</td>\n",
       "      <td>807.123762</td>\n",
       "      <td>710.721942</td>\n",
       "      <td>16.392533</td>\n",
       "      <td>3.751603</td>\n",
       "      <td>735.528417</td>\n",
       "      <td>680.055280</td>\n",
       "      <td>13.747434</td>\n",
       "      <td>3.476420</td>\n",
       "      <td>591.037583</td>\n",
       "      <td>523.503586</td>\n",
       "      <td>12.134629</td>\n",
       "      <td>3.170857</td>\n",
       "      <td>351.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>7.065548</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>4.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2.061453</td>\n",
       "      <td>2.061453</td>\n",
       "      <td>123.6872</td>\n",
       "      <td>0.2813</td>\n",
       "      <td>1.1523</td>\n",
       "      <td>40.7951</td>\n",
       "      <td>0.007</td>\n",
       "      <td>90</td>\n",
       "      <td>0.495223</td>\n",
       "      <td>6.893743</td>\n",
       "      <td>0.890445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1124</td>\n",
       "      <td>-16.543753</td>\n",
       "      <td>143.600189</td>\n",
       "      <td>7.145702</td>\n",
       "      <td>1.141288</td>\n",
       "      <td>20.051722</td>\n",
       "      <td>4.406298</td>\n",
       "      <td>0.695277</td>\n",
       "      <td>11.383690</td>\n",
       "      <td>2.753004</td>\n",
       "      <td>2.214854</td>\n",
       "      <td>1.933837</td>\n",
       "      <td>1.794938</td>\n",
       "      <td>0.173295</td>\n",
       "      <td>3.432418e+04</td>\n",
       "      <td>7.868462</td>\n",
       "      <td>3.015599e+06</td>\n",
       "      <td>7.908174</td>\n",
       "      <td>87.856390</td>\n",
       "      <td>160.143942</td>\n",
       "      <td>22.411225</td>\n",
       "      <td>1.822792</td>\n",
       "      <td>41.639721</td>\n",
       "      <td>32.987125</td>\n",
       "      <td>0.822496</td>\n",
       "      <td>-0.332169</td>\n",
       "      <td>268.808929</td>\n",
       "      <td>207.812015</td>\n",
       "      <td>6.112295</td>\n",
       "      <td>2.377222</td>\n",
       "      <td>594.150153</td>\n",
       "      <td>498.509820</td>\n",
       "      <td>10.343254</td>\n",
       "      <td>3.075437</td>\n",
       "      <td>643.020183</td>\n",
       "      <td>555.512641</td>\n",
       "      <td>14.095862</td>\n",
       "      <td>3.603208</td>\n",
       "      <td>574.553907</td>\n",
       "      <td>524.107264</td>\n",
       "      <td>16.377058</td>\n",
       "      <td>3.904008</td>\n",
       "      <td>393.114268</td>\n",
       "      <td>357.907185</td>\n",
       "      <td>14.434470</td>\n",
       "      <td>3.657305</td>\n",
       "      <td>352.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>6.727352</td>\n",
       "      <td>0.012543</td>\n",
       "      <td>10.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>2.231855</td>\n",
       "      <td>2.231855</td>\n",
       "      <td>133.9113</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>40.4166</td>\n",
       "      <td>0.024</td>\n",
       "      <td>90</td>\n",
       "      <td>0.395162</td>\n",
       "      <td>-1.928064</td>\n",
       "      <td>0.245788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id     flux_min    flux_max   flux_mean  flux_median    flux_std  \\\n",
       "0        615 -1100.440063  660.626343 -123.096998   -89.477524  394.109851   \n",
       "1        713   -14.735178   14.770886   -1.423351    -0.873033    6.471144   \n",
       "2        730   -19.159811   47.310059    2.267434     0.409172    8.022239   \n",
       "3        745   -15.494463  220.795212    8.909206     1.035895   27.558208   \n",
       "4       1124   -16.543753  143.600189    7.145702     1.141288   20.051722   \n",
       "\n",
       "   flux_skew  flux_err_min  flux_err_max  flux_err_mean  flux_err_median  \\\n",
       "0  -0.349540      2.130510     12.845472       4.482743         3.835269   \n",
       "1   0.014989      0.639458      9.115748       2.359620         1.998217   \n",
       "2   3.177854      0.695106     11.281384       2.471061         1.990851   \n",
       "3   4.979826      0.567170     55.892746       2.555576         1.819875   \n",
       "4   4.406298      0.695277     11.383690       2.753004         2.214854   \n",
       "\n",
       "   flux_err_std  flux_err_skew  detected_mean  flux_ratio_sq_sum  \\\n",
       "0      1.744747       1.623740       0.946023       2.929669e+06   \n",
       "1      1.509888       1.633246       0.171429       5.886068e+03   \n",
       "2      1.721134       1.823726       0.069697       4.124452e+03   \n",
       "3      3.537324      10.741655       0.173789       9.416165e+04   \n",
       "4      1.933837       1.794938       0.173295       3.432418e+04   \n",
       "\n",
       "   flux_ratio_sq_skew  flux_by_flux_ratio_sq_sum  flux_by_flux_ratio_sq_skew  \\\n",
       "0            0.812722              -9.601766e+08                   -1.414322   \n",
       "1            3.439423              -2.875087e+04                   -3.454554   \n",
       "2            5.480405               1.046502e+05                    5.989138   \n",
       "3            9.611274               1.439125e+07                   11.141069   \n",
       "4            7.868462               3.015599e+06                    7.908174   \n",
       "\n",
       "   flux_w_mean   flux_diff1  flux_diff2  flux_diff3  \\\n",
       "0  -327.742307  1761.066406  -14.306331   -5.373326   \n",
       "1    -4.884564    29.506064  -20.730002   -6.040676   \n",
       "2    25.373110    66.469870   29.315018    2.619697   \n",
       "3   152.835617   236.289675   26.521968    1.546038   \n",
       "4    87.856390   160.143942   22.411225    1.822792   \n",
       "\n",
       "   0__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                               205.036926   \n",
       "1                               190.427851   \n",
       "2                                 3.461790   \n",
       "3                               129.421659   \n",
       "4                                41.639721   \n",
       "\n",
       "   0__fft_coefficient__coeff_1__attr_\"abs\"  0__kurtosis  0__skewness  \\\n",
       "0                              1628.427737    -1.475181     0.128917   \n",
       "1                               299.586559    -1.014003     0.260052   \n",
       "2                                 4.729538     0.474215     0.356910   \n",
       "3                               123.298327     4.629801     2.023211   \n",
       "4                                32.987125     0.822496    -0.332169   \n",
       "\n",
       "   1__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                             22370.594834   \n",
       "1                                57.109047   \n",
       "2                                 7.334944   \n",
       "3                               320.174052   \n",
       "4                               268.808929   \n",
       "\n",
       "   1__fft_coefficient__coeff_1__attr_\"abs\"  1__kurtosis  1__skewness  \\\n",
       "0                              2806.374162    -1.255123     0.415580   \n",
       "1                               192.539229    -1.097170    -0.087865   \n",
       "2                                13.515895     0.976374     0.471342   \n",
       "3                               280.440312    50.868880     7.007099   \n",
       "4                               207.812015     6.112295     2.377222   \n",
       "\n",
       "   2__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                              7780.500807   \n",
       "1                                44.477327   \n",
       "2                               124.845250   \n",
       "3                               543.845781   \n",
       "4                               594.150153   \n",
       "\n",
       "   2__fft_coefficient__coeff_1__attr_\"abs\"  2__kurtosis  2__skewness  \\\n",
       "0                              2805.598113    -1.409885     0.339918   \n",
       "1                               191.057528    -1.188472    -0.022678   \n",
       "2                               119.500254     5.131290     2.385066   \n",
       "3                               491.548270    36.088137     5.688194   \n",
       "4                               498.509820    10.343254     3.075437   \n",
       "\n",
       "   3__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                              7024.003068   \n",
       "1                                55.270113   \n",
       "2                               168.280524   \n",
       "3                               807.123762   \n",
       "4                               643.020183   \n",
       "\n",
       "   3__fft_coefficient__coeff_1__attr_\"abs\"  3__kurtosis  3__skewness  \\\n",
       "0                              2536.068846    -1.449858     0.293128   \n",
       "1                               212.522263    -1.142896    -0.167176   \n",
       "2                               162.799417     7.125665     2.662075   \n",
       "3                               710.721942    16.392533     3.751603   \n",
       "4                               555.512641    14.095862     3.603208   \n",
       "\n",
       "   4__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                              3245.366349   \n",
       "1                                50.414646   \n",
       "2                               219.745132   \n",
       "3                               735.528417   \n",
       "4                               574.553907   \n",
       "\n",
       "   4__fft_coefficient__coeff_1__attr_\"abs\"  4__kurtosis  4__skewness  \\\n",
       "0                              2741.539785    -1.548319     0.200096   \n",
       "1                               203.892482    -1.190245    -0.064134   \n",
       "2                               202.532898     6.081065     2.537802   \n",
       "3                               680.055280    13.747434     3.476420   \n",
       "4                               524.107264    16.377058     3.904008   \n",
       "\n",
       "   5__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                              2704.641265   \n",
       "1                               100.473776   \n",
       "2                               231.509177   \n",
       "3                               591.037583   \n",
       "4                               393.114268   \n",
       "\n",
       "   5__fft_coefficient__coeff_1__attr_\"abs\"  5__kurtosis  5__skewness  \\\n",
       "0                              2893.344217    -1.592820     0.125268   \n",
       "1                               143.963093    -0.797047     0.218182   \n",
       "2                               199.286370     3.583130     1.680352   \n",
       "3                               523.503586    12.134629     3.170857   \n",
       "4                               357.907185    14.434470     3.657305   \n",
       "\n",
       "   flux__length  flux__longest_strike_above_mean  \\\n",
       "0         352.0                             19.0   \n",
       "1         350.0                             50.0   \n",
       "2         330.0                             13.0   \n",
       "3         351.0                             19.0   \n",
       "4         352.0                             19.0   \n",
       "\n",
       "   flux__longest_strike_below_mean  flux__mean_abs_change  flux__mean_change  \\\n",
       "0                             29.0             202.114067           1.999688   \n",
       "1                             73.0               2.935177          -0.050944   \n",
       "2                             32.0               4.227614          -0.008131   \n",
       "3                            115.0               7.065548           0.008044   \n",
       "4                            158.0               6.727352           0.012543   \n",
       "\n",
       "   flux_by_flux_ratio_sq__longest_strike_above_mean  \\\n",
       "0                                              35.0   \n",
       "1                                             199.0   \n",
       "2                                               4.0   \n",
       "3                                               4.0   \n",
       "4                                              10.0   \n",
       "\n",
       "   flux_by_flux_ratio_sq__longest_strike_below_mean  mjd__mean_abs_change  \\\n",
       "0                                               4.0              2.631898   \n",
       "1                                               8.0             14.352571   \n",
       "2                                             222.0              3.580623   \n",
       "3                                             201.0              2.061453   \n",
       "4                                             231.0              2.231855   \n",
       "\n",
       "   mjd__mean_change  mjd_diff_det  hostgal_photoz  hostgal_photoz_err  \\\n",
       "0          2.631898      873.7903          0.0000              0.0000   \n",
       "1         14.352571      846.8017          1.6267              0.2552   \n",
       "2          3.580623       78.7737          0.2262              0.0157   \n",
       "3          2.061453      123.6872          0.2813              1.1523   \n",
       "4          2.231855      133.9113          0.2415              0.0176   \n",
       "\n",
       "   distmod  mwebv  target  haversine   latlon1  hostgal_photoz_certain  \n",
       "0      NaN  0.017      92   0.319006 -1.528827                0.000000  \n",
       "1  45.4063  0.007      88   1.698939  3.258921                2.099614  \n",
       "2  40.2561  0.021      42   1.818030  3.128522                0.229779  \n",
       "3  40.7951  0.007      90   0.495223  6.893743                0.890445  \n",
       "4  40.4166  0.024      90   0.395162 -1.928064                0.245788  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = full_train.mean(axis=0)\n",
    "#train_mean.to_hdf('train_data.hdf5', 'data')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "#import pdb; pdb.set_trace()\n",
    "full_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "klm.to_csv('klm_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(full_train.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_func = partial(lgbm_modeling_cross_validation,\n",
    "                    full_train=full_train,\n",
    "                    y=y,\n",
    "                    classes=classes,\n",
    "                    class_weights=class_weights,\n",
    "                    nr_fold=5,\n",
    "                    random_state=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params.update({'n_estimators': 1000})\n",
    "\n",
    "# modeling from CV\n",
    "clfs, score = eval_func(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'subm_{:.6f}_{}.csv'.format(score,\n",
    "                 dt.now().strftime('%Y-%m-%d-%H-%M'))\n",
    "print('save to {}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "process_test(clfs,\n",
    "             features=full_train.columns,\n",
    "             featurize_configs={'aggs': aggs, 'fcp': fcp},\n",
    "             train_mean=train_mean,\n",
    "             filename=filename,\n",
    "             chunks=5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape BEFORE grouping: {}\".format(z.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z = z.groupby('object_id').mean()\n",
    "print(\"Shape AFTER grouping: {}\".format(z.shape))\n",
    "z.to_csv('single_{}'.format(filename), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "v = dd.read_csv('klm_test.csv')\n",
    "v = v.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(klm.columns)) == sorted(list(v.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
