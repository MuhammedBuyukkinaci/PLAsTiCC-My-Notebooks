{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "np.warnings.filterwarnings('ignore')\n",
    "import dask.dataframe as dd\n",
    "import missingno as msno\n",
    "from pandasql import sqldf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always seed the randomness of this universe\n",
    "np.random.seed(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 139) (3492890, 138)\n",
      "CPU times: user 3min 20s, sys: 11.8 s, total: 3min 32s\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_metadata_kaggle = dd.read_csv('mydata_train_metadata.csv')\n",
    "test_metadata_kaggle = dd.read_csv('mydata_test_metadata.csv')\n",
    "train_metadata_kaggle = train_metadata_kaggle.compute()\n",
    "test_metadata_kaggle = test_metadata_kaggle.compute()\n",
    "print(train_metadata_kaggle.shape,test_metadata_kaggle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.5 s, sys: 3.39 s, total: 5.89 s\n",
      "Wall time: 5.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_metadata_kaggle = test_metadata_kaggle.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_metadata_kaggle['object_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 139)\n",
      "0 0.5846232690590423\n",
      "1 0.5598478406280183\n",
      "2 0.6060594241195905\n",
      "3 0.5551329424743721\n",
      "4 0.5738283897556737\n",
      "MULTI WEIGHTED LOG LOSS : 0.57588 \n",
      "CPU times: user 11min 34s, sys: 1.73 s, total: 11min 36s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "\n",
    "loss_list = []\n",
    "temp = train_metadata_kaggle.copy()\n",
    "#temp = temp.merge(train_metadata[['object_id'] + used_columns1],on = 'object_id',how = 'left')\n",
    "print(temp.shape)\n",
    "#temp = temp.merge(train_metadata[['object_id',column_]],on = 'object_id',how = 'left')\n",
    "y = temp['target']\n",
    "del temp['target']\n",
    "classes = sorted(y.unique())\n",
    "\n",
    "# Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "# with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "class_weight = {\n",
    "    c: 1 for c in classes\n",
    "}\n",
    "for c in [64, 15]:\n",
    "    class_weight[c] = 2\n",
    "\n",
    "#print('Unique classes : ', classes)\n",
    "\n",
    "train_id = temp['object_id']\n",
    "del temp['object_id']\n",
    "# Compute weights\n",
    "w = y.value_counts()\n",
    "weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "clfs = []\n",
    "importances = pd.DataFrame()\n",
    "lgb_params = {\n",
    "'random_state':51,\n",
    "'device': 'cpu', \n",
    "'objective': 'multiclass', \n",
    "'num_class': 14, \n",
    "'boosting_type': 'gbdt', \n",
    "'n_jobs': -1, \n",
    "#'max_depth': 7, \n",
    "'n_estimators': 1000, \n",
    "'subsample_freq': 2, \n",
    "'subsample_for_bin': 5000, \n",
    "'min_data_per_group': 100, \n",
    "'max_cat_to_onehot': 4, \n",
    "'cat_l2': 1.0, \n",
    "'cat_smooth': 59.5, \n",
    "'max_cat_threshold': 32, \n",
    "'metric_freq': 10, \n",
    "'verbosity': -1, \n",
    "'metric': 'multi_logloss', \n",
    "'xgboost_dart_mode': False, \n",
    "'uniform_drop': False, \n",
    "'colsample_bytree': 0.5, \n",
    "'drop_rate': 0.173, \n",
    "'learning_rate': 0.0267, \n",
    "'max_drop': 5, \n",
    "'min_child_samples': 10,\n",
    "'min_child_weight': 200.0, \n",
    "#'min_child_weight': 100.0, \n",
    "'min_split_gain': 0.1, \n",
    "'num_leaves': 7, \n",
    "#'reg_alpha': 0.1,\n",
    "'reg_alpha': 0.0, \n",
    "'reg_lambda': 0.00023, \n",
    "'skip_drop': 0.44, \n",
    "'subsample': 0.75}\n",
    "oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "    trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "    val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "    clf = lgb.LGBMClassifier(**lgb_params)\n",
    "    clf.fit(\n",
    "        trn_x, trn_y,\n",
    "        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "        eval_metric=lgb_multi_weighted_logloss,\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=50,\n",
    "        sample_weight=trn_y.map(weights)\n",
    "    )\n",
    "    oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "    loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "    #loss_list.append(loss_oof)\n",
    "    print(fold_,loss_oof)\n",
    "\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = temp.columns\n",
    "    imp_df['gain'] = clf.feature_importances_\n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    clfs.append(clf)\n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "#final_dict[column_] = loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 139)\n",
      "2 [0.5846232690590423, 0.5598478406280183, 0.6060594241195905, 0.5551329424743721, 0.5738283897556737]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57588 \n",
      "(7848, 139)\n",
      "3 [0.5812347980144251, 0.558722069534653, 0.6094043217820466, 0.5643351744809683, 0.57406946929533]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57754 \n",
      "(7848, 139)\n",
      "4 [0.5795408254189881, 0.5621103145857294, 0.6087282931399269, 0.5634614254711744, 0.567038047228325]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57616 \n",
      "(7848, 139)\n",
      "5 [0.5818531991668668, 0.5566890683011684, 0.6086201149905734, 0.5614331255465247, 0.5663315357166874]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57500 \n",
      "(7848, 139)\n",
      "6 [0.577211069460461, 0.5579433752331334, 0.6021804408687527, 0.5656735512550161, 0.5719369423290517]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57495 \n"
     ]
    }
   ],
   "source": [
    "final_dict = {}\n",
    "parameter_tuned = 'subsample_freq'\n",
    "for i in [2,3,4,5,6]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 1000, \n",
    "    'subsample_freq': i, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 10,\n",
    "    'min_child_weight': 200.0, \n",
    "    #'min_child_weight': 100.0, \n",
    "    'min_split_gain': 0.1, \n",
    "    'num_leaves': 7, \n",
    "    #'reg_alpha': 0.1,\n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,6))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    [0.5846232690590423, 0.5598478406280183, 0.606...\n",
       "3    [0.5812347980144251, 0.558722069534653, 0.6094...\n",
       "4    [0.5795408254189881, 0.5621103145857294, 0.608...\n",
       "5    [0.5818531991668668, 0.5566890683011684, 0.608...\n",
       "6    [0.577211069460461, 0.5579433752331334, 0.6021...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 [0.583214, 0.564629, 0.6046, 0.5667, 0.56794]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57747 \n",
      "3000 [0.580759, 0.561566, 0.60691, 0.567888, 0.569603]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57735 \n",
      "4000 [0.583355, 0.560267, 0.612454, 0.564482, 0.568709]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57788 \n",
      "5000 [0.577211, 0.557943, 0.60218, 0.565674, 0.571937]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57495 \n",
      "6000 [0.580719, 0.558598, 0.608918, 0.571142, 0.568037]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57748 \n",
      "subsample_freq_2000        [0.583214, 0.564629, 0.6046, 0.5667, 0.56794]\n",
      "subsample_freq_3000    [0.580759, 0.561566, 0.60691, 0.567888, 0.569603]\n",
      "subsample_freq_4000    [0.583355, 0.560267, 0.612454, 0.564482, 0.568...\n",
      "subsample_freq_5000    [0.577211, 0.557943, 0.60218, 0.565674, 0.571937]\n",
      "subsample_freq_6000    [0.580719, 0.558598, 0.608918, 0.571142, 0.568...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "parameter_tuned = 'subsample_freq'\n",
    "for i in [2000,3000,4000,5000,6000]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 1000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': i, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 10,\n",
    "    'min_child_weight': 200.0, \n",
    "    #'min_child_weight': 100.0, \n",
    "    'min_split_gain': 0.1, \n",
    "    'num_leaves': 7, \n",
    "    #'reg_alpha': 0.1,\n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57495 \n",
      "5 [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57495 \n",
      "10 [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57495 \n",
      "20 [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57495 \n",
      "50 [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57495 \n",
      "metric_freq_1     [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "metric_freq_5     [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "metric_freq_10    [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "metric_freq_20    [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "metric_freq_50    [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "dtype: object\n",
      "CPU times: user 1h 1min 57s, sys: 12.4 s, total: 1h 2min 9s\n",
      "Wall time: 8min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "parameter_tuned = 'metric_freq'\n",
    "for i in [1,5,10,20,50]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 1000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 10,\n",
    "    'min_child_weight': 200.0, \n",
    "    #'min_child_weight': 100.0, \n",
    "    'min_split_gain': 0.1, \n",
    "    'num_leaves': 7, \n",
    "    #'reg_alpha': 0.1,\n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 [0.58174, 0.55887, 0.60791, 0.56649, 0.57043]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57711 \n",
      "0.5 [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57495 \n",
      "0.6 [0.57435, 0.5588, 0.61011, 0.56269, 0.57587]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57634 \n",
      "0.7 [0.57602, 0.5622, 0.61196, 0.56568, 0.57914]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57896 \n",
      "0.8 [0.5767, 0.56184, 0.61859, 0.56897, 0.58117]\n",
      "MULTI WEIGHTED LOG LOSS : 0.58139 \n",
      "0.9 [0.57998, 0.56492, 0.61784, 0.57321, 0.58108]\n",
      "MULTI WEIGHTED LOG LOSS : 0.58336 \n",
      "colsample_bytree_0.4    [0.58174, 0.55887, 0.60791, 0.56649, 0.57043]\n",
      "colsample_bytree_0.5    [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "colsample_bytree_0.6     [0.57435, 0.5588, 0.61011, 0.56269, 0.57587]\n",
      "colsample_bytree_0.7     [0.57602, 0.5622, 0.61196, 0.56568, 0.57914]\n",
      "colsample_bytree_0.8     [0.5767, 0.56184, 0.61859, 0.56897, 0.58117]\n",
      "colsample_bytree_0.9    [0.57998, 0.56492, 0.61784, 0.57321, 0.58108]\n",
      "dtype: object\n",
      "CPU times: user 1h 23min 14s, sys: 17.1 s, total: 1h 23min 31s\n",
      "Wall time: 11min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "parameter_tuned = 'colsample_bytree'\n",
    "for i in [0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 1000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': i, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 10,\n",
    "    'min_child_weight': 200.0, \n",
    "    #'min_child_weight': 100.0, \n",
    "    'min_split_gain': 0.1, \n",
    "    'num_leaves': 7, \n",
    "    #'reg_alpha': 0.1,\n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 [0.57821, 0.56288, 0.61278, 0.56149, 0.57077]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57722 \n",
      "0.015 [0.57744, 0.56311, 0.60913, 0.56052, 0.56951]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57597 \n",
      "0.02 [0.58188, 0.55953, 0.61009, 0.56367, 0.57297]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57764 \n",
      "0.025 [0.57482, 0.55745, 0.60672, 0.57016, 0.57165]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57609 \n",
      "0.03 [0.57289, 0.56307, 0.60392, 0.56798, 0.57133]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57579 \n",
      "0.035 [0.57773, 0.56208, 0.6028, 0.56419, 0.57361]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57605 \n",
      "learning_rate_0.01     [0.57821, 0.56288, 0.61278, 0.56149, 0.57077]\n",
      "learning_rate_0.015    [0.57744, 0.56311, 0.60913, 0.56052, 0.56951]\n",
      "learning_rate_0.02     [0.58188, 0.55953, 0.61009, 0.56367, 0.57297]\n",
      "learning_rate_0.025    [0.57482, 0.55745, 0.60672, 0.57016, 0.57165]\n",
      "learning_rate_0.03     [0.57289, 0.56307, 0.60392, 0.56798, 0.57133]\n",
      "learning_rate_0.035     [0.57773, 0.56208, 0.6028, 0.56419, 0.57361]\n",
      "dtype: object\n",
      "CPU times: user 1h 38min 36s, sys: 21.5 s, total: 1h 38min 58s\n",
      "Wall time: 13min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#0.0267\n",
    "parameter_tuned = 'learning_rate'\n",
    "for i in [0.010,0.015,0.020,0.025,0.030,0.035]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': i, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 10,\n",
    "    'min_child_weight': 200.0, \n",
    "    #'min_child_weight': 100.0, \n",
    "    'min_split_gain': 0.1, \n",
    "    'num_leaves': 7, \n",
    "    #'reg_alpha': 0.1,\n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [0.57613, 0.55804, 0.60028, 0.56498, 0.57032]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57390 \n",
      "10 [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57495 \n",
      "20 [0.57721, 0.56012, 0.59885, 0.5658, 0.56983]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57433 \n",
      "30 [0.57787, 0.55954, 0.60001, 0.56433, 0.56959]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57423 \n",
      "40 [0.5773, 0.55988, 0.5937, 0.56723, 0.57061]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57372 \n",
      "min_child_samples_5     [0.57613, 0.55804, 0.60028, 0.56498, 0.57032]\n",
      "min_child_samples_10    [0.57721, 0.55794, 0.60218, 0.56567, 0.57194]\n",
      "min_child_samples_20     [0.57721, 0.56012, 0.59885, 0.5658, 0.56983]\n",
      "min_child_samples_30    [0.57787, 0.55954, 0.60001, 0.56433, 0.56959]\n",
      "min_child_samples_40      [0.5773, 0.55988, 0.5937, 0.56723, 0.57061]\n",
      "dtype: object\n",
      "CPU times: user 48min 3s, sys: 7 s, total: 48min 10s\n",
      "Wall time: 6min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#10\n",
    "parameter_tuned = 'min_child_samples'\n",
    "for i in [5,10,20,30,40]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    #'min_child_samples': 10,\n",
    "    'min_child_samples': i,\n",
    "    'min_child_weight': 200.0, \n",
    "    #'min_child_weight': 100.0, \n",
    "    'min_split_gain': 0.1, \n",
    "    'num_leaves': 7, \n",
    "    #'reg_alpha': 0.1,\n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [0.58603, 0.55264, 0.60075, 0.56095, 0.56961]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57397 \n",
      "150 [0.57981, 0.55451, 0.59484, 0.56493, 0.56796]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57238 \n",
      "200 [0.5773, 0.55988, 0.5937, 0.56723, 0.57061]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57372 \n",
      "250 [0.58225, 0.56556, 0.601, 0.56785, 0.5732]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57797 \n",
      "300 [0.58125, 0.57181, 0.60151, 0.57276, 0.57158]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57982 \n",
      "min_child_weight_100    [0.58603, 0.55264, 0.60075, 0.56095, 0.56961]\n",
      "min_child_weight_150    [0.57981, 0.55451, 0.59484, 0.56493, 0.56796]\n",
      "min_child_weight_200      [0.5773, 0.55988, 0.5937, 0.56723, 0.57061]\n",
      "min_child_weight_250       [0.58225, 0.56556, 0.601, 0.56785, 0.5732]\n",
      "min_child_weight_300    [0.58125, 0.57181, 0.60151, 0.57276, 0.57158]\n",
      "dtype: object\n",
      "CPU times: user 41min 5s, sys: 5.6 s, total: 41min 10s\n",
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#200\n",
    "parameter_tuned = 'min_child_weight'\n",
    "for i in [100,150,200,250,300]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    #'min_child_samples': 10,\n",
    "    'min_child_samples': 40,\n",
    "    'min_child_weight': i, \n",
    "    #'min_child_weight': 200.0, \n",
    "    'min_split_gain': 0.1, \n",
    "    'num_leaves': 7, \n",
    "    #'reg_alpha': 0.1,\n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 [0.5815, 0.55448, 0.594, 0.56581, 0.56776]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57269 \n",
      "0.1 [0.57981, 0.55451, 0.59484, 0.56493, 0.56796]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57238 \n",
      "0.15 [0.58143, 0.55315, 0.59422, 0.5652, 0.56725]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57223 \n",
      "0.2 [0.58367, 0.55381, 0.59515, 0.56415, 0.56677]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57270 \n",
      "min_split_gain_0.05       [0.5815, 0.55448, 0.594, 0.56581, 0.56776]\n",
      "min_split_gain_0.1     [0.57981, 0.55451, 0.59484, 0.56493, 0.56796]\n",
      "min_split_gain_0.15     [0.58143, 0.55315, 0.59422, 0.5652, 0.56725]\n",
      "min_split_gain_0.2     [0.58367, 0.55381, 0.59515, 0.56415, 0.56677]\n",
      "dtype: object\n",
      "CPU times: user 49min 52s, sys: 9.29 s, total: 50min 1s\n",
      "Wall time: 6min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#200\n",
    "parameter_tuned = 'min_split_gain'\n",
    "for i in [0.05,0.1,0.15,0.20]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 40,\n",
    "    'min_child_weight': 150, \n",
    "    'min_split_gain': i, \n",
    "    'num_leaves': 7, \n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 [0.58143, 0.55315, 0.59422, 0.5652, 0.56725]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57223 \n",
      "15 [0.59159, 0.58486, 0.61192, 0.58034, 0.58697]\n",
      "MULTI WEIGHTED LOG LOSS : 0.59112 \n",
      "25 [0.59974, 0.59089, 0.62314, 0.58964, 0.59799]\n",
      "MULTI WEIGHTED LOG LOSS : 0.60024 \n",
      "35 [0.59917, 0.58728, 0.62374, 0.59459, 0.59852]\n",
      "MULTI WEIGHTED LOG LOSS : 0.60064 \n",
      "45 [0.60018, 0.58814, 0.62054, 0.59383, 0.59798]\n",
      "MULTI WEIGHTED LOG LOSS : 0.60009 \n",
      "65 [0.60018, 0.58814, 0.62054, 0.59383, 0.59798]\n",
      "MULTI WEIGHTED LOG LOSS : 0.60009 \n",
      "num_leaves_7      [0.58143, 0.55315, 0.59422, 0.5652, 0.56725]\n",
      "num_leaves_15    [0.59159, 0.58486, 0.61192, 0.58034, 0.58697]\n",
      "num_leaves_25    [0.59974, 0.59089, 0.62314, 0.58964, 0.59799]\n",
      "num_leaves_35    [0.59917, 0.58728, 0.62374, 0.59459, 0.59852]\n",
      "num_leaves_45    [0.60018, 0.58814, 0.62054, 0.59383, 0.59798]\n",
      "num_leaves_65    [0.60018, 0.58814, 0.62054, 0.59383, 0.59798]\n",
      "dtype: object\n",
      "CPU times: user 48min 37s, sys: 9.52 s, total: 48min 47s\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#7\n",
    "parameter_tuned = 'num_leaves'\n",
    "for i in [7,15,25,35,45,65]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 40,\n",
    "    'min_child_weight': 150, \n",
    "    'min_split_gain': 0.15, \n",
    "    'num_leaves': i, \n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [0.57984, 0.55343, 0.59763, 0.56107, 0.5707]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57253 \n",
      "7 [0.58143, 0.55315, 0.59422, 0.5652, 0.56725]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57223 \n",
      "9 [0.57986, 0.56351, 0.59991, 0.57124, 0.57725]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57834 \n",
      "11 [0.58753, 0.57099, 0.60249, 0.57121, 0.58181]\n",
      "MULTI WEIGHTED LOG LOSS : 0.58280 \n",
      "num_leaves_5      [0.57984, 0.55343, 0.59763, 0.56107, 0.5707]\n",
      "num_leaves_7      [0.58143, 0.55315, 0.59422, 0.5652, 0.56725]\n",
      "num_leaves_9     [0.57986, 0.56351, 0.59991, 0.57124, 0.57725]\n",
      "num_leaves_11    [0.58753, 0.57099, 0.60249, 0.57121, 0.58181]\n",
      "dtype: object\n",
      "CPU times: user 31min 53s, sys: 4.51 s, total: 31min 57s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#7\n",
    "parameter_tuned = 'num_leaves'\n",
    "for i in [5,7,9,11]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 40,\n",
    "    'min_child_weight': 150, \n",
    "    'min_split_gain': 0.15, \n",
    "    'num_leaves': i, \n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [0.57999, 0.54996, 0.59388, 0.56439, 0.56667]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57099 \n",
      "8 [0.581, 0.5564, 0.59821, 0.56681, 0.57524]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57551 \n",
      "num_leaves_6    [0.57999, 0.54996, 0.59388, 0.56439, 0.56667]\n",
      "num_leaves_8       [0.581, 0.5564, 0.59821, 0.56681, 0.57524]\n",
      "dtype: object\n",
      "CPU times: user 16min 33s, sys: 2.19 s, total: 16min 35s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#7\n",
    "parameter_tuned = 'num_leaves'\n",
    "for i in [6,8]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 40,\n",
    "    'min_child_weight': 150, \n",
    "    'min_split_gain': 0.15, \n",
    "    'num_leaves': i, \n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [0.57999, 0.54996, 0.59388, 0.56439, 0.56667]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57099 \n",
      "0.1 [0.5764, 0.54995, 0.59254, 0.56548, 0.56649]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57015 \n",
      "0.5 [0.57755, 0.55161, 0.59464, 0.56404, 0.56691]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57095 \n",
      "1 [0.57801, 0.55004, 0.59359, 0.56505, 0.56685]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57071 \n",
      "5 [0.57933, 0.55036, 0.59298, 0.56286, 0.56855]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57081 \n",
      "10 [0.57759, 0.55286, 0.59648, 0.56105, 0.56955]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57151 \n",
      "100 [0.58838, 0.57936, 0.62427, 0.58124, 0.5892]\n",
      "MULTI WEIGHTED LOG LOSS : 0.59248 \n",
      "reg_alpha_0.0    [0.57999, 0.54996, 0.59388, 0.56439, 0.56667]\n",
      "reg_alpha_0.1     [0.5764, 0.54995, 0.59254, 0.56548, 0.56649]\n",
      "reg_alpha_0.5    [0.57755, 0.55161, 0.59464, 0.56404, 0.56691]\n",
      "reg_alpha_1      [0.57801, 0.55004, 0.59359, 0.56505, 0.56685]\n",
      "reg_alpha_5      [0.57933, 0.55036, 0.59298, 0.56286, 0.56855]\n",
      "reg_alpha_10     [0.57759, 0.55286, 0.59648, 0.56105, 0.56955]\n",
      "reg_alpha_100     [0.58838, 0.57936, 0.62427, 0.58124, 0.5892]\n",
      "dtype: object\n",
      "CPU times: user 1h 38min 57s, sys: 15.8 s, total: 1h 39min 12s\n",
      "Wall time: 13min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#7\n",
    "parameter_tuned = 'reg_alpha'\n",
    "for i in [0.0,0.1,0.5,1,5,10,100]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 40,\n",
    "    'min_child_weight': 150, \n",
    "    'min_split_gain': 0.15, \n",
    "    'num_leaves': 6, \n",
    "    'reg_alpha': i, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001 [0.5764, 0.54995, 0.59254, 0.56376, 0.56649]\n",
      "MULTI WEIGHTED LOG LOSS : 0.56981 \n",
      "0.00023 [0.5764, 0.54995, 0.59254, 0.56548, 0.56649]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57015 \n",
      "0.0005 [0.5764, 0.54995, 0.59261, 0.56548, 0.5659]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57005 \n",
      "0.001 [0.5764, 0.54968, 0.59261, 0.56469, 0.5659]\n",
      "MULTI WEIGHTED LOG LOSS : 0.56984 \n",
      "0.005 [0.57847, 0.54941, 0.59232, 0.56743, 0.56645]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57083 \n",
      "0.01 [0.57834, 0.54906, 0.59288, 0.56433, 0.56517]\n",
      "MULTI WEIGHTED LOG LOSS : 0.56995 \n",
      "reg_lambda_0.0001      [0.5764, 0.54995, 0.59254, 0.56376, 0.56649]\n",
      "reg_lambda_0.00023     [0.5764, 0.54995, 0.59254, 0.56548, 0.56649]\n",
      "reg_lambda_0.0005       [0.5764, 0.54995, 0.59261, 0.56548, 0.5659]\n",
      "reg_lambda_0.001        [0.5764, 0.54968, 0.59261, 0.56469, 0.5659]\n",
      "reg_lambda_0.005      [0.57847, 0.54941, 0.59232, 0.56743, 0.56645]\n",
      "reg_lambda_0.01       [0.57834, 0.54906, 0.59288, 0.56433, 0.56517]\n",
      "dtype: object\n",
      "CPU times: user 51min 9s, sys: 6.43 s, total: 51min 15s\n",
      "Wall time: 6min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#7\n",
    "parameter_tuned = 'reg_lambda'\n",
    "for i in [0.0001,0.00023,0.0005,0.001,0.005,0.01]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 40,\n",
    "    'min_child_weight': 150, \n",
    "    'min_split_gain': 0.15, \n",
    "    'num_leaves': 6, \n",
    "    'reg_alpha': 0.1, \n",
    "    'reg_lambda': i, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45 [0.59853, 0.57488, 0.59784, 0.57983, 0.57539]\n",
      "MULTI WEIGHTED LOG LOSS : 0.58531 \n",
      "0.55 [0.5868, 0.55992, 0.58625, 0.57378, 0.56725]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57482 \n",
      "0.65 [0.58232, 0.55812, 0.59304, 0.57077, 0.56938]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57474 \n",
      "0.75 [0.5764, 0.54968, 0.59261, 0.56469, 0.5659]\n",
      "MULTI WEIGHTED LOG LOSS : 0.56984 \n",
      "0.85 [0.58654, 0.55706, 0.5915, 0.55684, 0.5743]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57330 \n",
      "0.95 [0.59008, 0.562, 0.59881, 0.56095, 0.57514]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57746 \n",
      "subsample_0.45    [0.59853, 0.57488, 0.59784, 0.57983, 0.57539]\n",
      "subsample_0.55     [0.5868, 0.55992, 0.58625, 0.57378, 0.56725]\n",
      "subsample_0.65    [0.58232, 0.55812, 0.59304, 0.57077, 0.56938]\n",
      "subsample_0.75      [0.5764, 0.54968, 0.59261, 0.56469, 0.5659]\n",
      "subsample_0.85      [0.58654, 0.55706, 0.5915, 0.55684, 0.5743]\n",
      "subsample_0.95      [0.59008, 0.562, 0.59881, 0.56095, 0.57514]\n",
      "dtype: object\n",
      "CPU times: user 50min 4s, sys: 6.35 s, total: 50min 10s\n",
      "Wall time: 6min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#7\n",
    "parameter_tuned = 'subsample'\n",
    "for i in [0.45,0.55,0.65,0.75,0.85,0.95]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 40,\n",
    "    'min_child_weight': 150, \n",
    "    'min_split_gain': 0.15, \n",
    "    'num_leaves': 6, \n",
    "    'reg_alpha': 0.1, \n",
    "    'reg_lambda': 0.001, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': i}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73 [0.57816, 0.5533, 0.58652, 0.56378, 0.56792]\n",
      "MULTI WEIGHTED LOG LOSS : 0.56997 \n",
      "0.74 [0.5783, 0.54968, 0.58956, 0.56561, 0.5665]\n",
      "MULTI WEIGHTED LOG LOSS : 0.56994 \n",
      "0.75 [0.5764, 0.54968, 0.59261, 0.56469, 0.5659]\n",
      "MULTI WEIGHTED LOG LOSS : 0.56984 \n",
      "0.76 [0.58192, 0.55302, 0.59044, 0.56234, 0.56849]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57125 \n",
      "0.77 [0.58279, 0.55087, 0.59781, 0.56471, 0.56965]\n",
      "MULTI WEIGHTED LOG LOSS : 0.57314 \n",
      "subsample_0.73     [0.57816, 0.5533, 0.58652, 0.56378, 0.56792]\n",
      "subsample_0.74      [0.5783, 0.54968, 0.58956, 0.56561, 0.5665]\n",
      "subsample_0.75      [0.5764, 0.54968, 0.59261, 0.56469, 0.5659]\n",
      "subsample_0.76    [0.58192, 0.55302, 0.59044, 0.56234, 0.56849]\n",
      "subsample_0.77    [0.58279, 0.55087, 0.59781, 0.56471, 0.56965]\n",
      "dtype: object\n",
      "CPU times: user 1h 6min 8s, sys: 9.95 s, total: 1h 6min 18s\n",
      "Wall time: 9min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "#7\n",
    "parameter_tuned = 'subsample'\n",
    "for i in [0.73,0.74,0.75,0.76,0.77]:\n",
    "    loss_list = []\n",
    "    temp = train_metadata_kaggle.copy()\n",
    "    \n",
    "    y = temp['target']\n",
    "    del temp['target']\n",
    "    classes = sorted(y.unique())\n",
    "\n",
    "    class_weight = {\n",
    "        c: 1 for c in classes\n",
    "    }\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "\n",
    "    train_id = temp['object_id']\n",
    "    del temp['object_id']\n",
    "    \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "    \n",
    "    lgb_params = {\n",
    "    'random_state':51,\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 4000, \n",
    "    'subsample_freq': 6, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 40,\n",
    "    'min_child_weight': 150, \n",
    "    'min_split_gain': 0.15, \n",
    "    'num_leaves': 6, \n",
    "    'reg_alpha': 0.1, \n",
    "    'reg_lambda': 0.001, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': i}\n",
    "    oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "        loss_list.append(round(loss_oof,5))\n",
    "        #print(fold_,loss_oof)\n",
    "\n",
    "    print(i,loss_list) \n",
    "    final_dict[parameter_tuned + '_' + str(i)] = loss_list\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "print(pd.Series(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_dict2[final_dict2['column_name'].isin(most_imp_ones)]\n",
    "#final_dict2[final_dict2['fold_sum'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_dict2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_dict2.to_csv('final_features3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify to work with kfold\n",
    "#def smoteAdataset(Xig, yig, test_size=0.2, random_state=0):\n",
    "def smoteAdataset(Xig_train, yig_train, Xig_test, yig_test):\n",
    "    \n",
    "        \n",
    "    sm=SMOTE(random_state=51)\n",
    "    Xig_train_res, yig_train_res = sm.fit_sample(Xig_train, yig_train.ravel())\n",
    "\n",
    "        \n",
    "    return Xig_train_res, pd.Series(yig_train_res), Xig_test, pd.Series(yig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 139)\n",
      "(25900, 137) (25900,) (1574, 137) (1574,)\n",
      "0 0.5407134604014688\n",
      "(25900, 137) (25900,) (1572, 137) (1572,)\n",
      "1 0.5505294469507865\n",
      "(25900, 137) (25900,) (1571, 137) (1571,)\n",
      "2 0.5930505215690117\n",
      "(25914, 137) (25914,) (1567, 137) (1567,)\n",
      "3 0.5412398994685668\n",
      "(25914, 137) (25914,) (1564, 137) (1564,)\n",
      "4 0.5650806630208185\n",
      "MULTI WEIGHTED LOG LOSS : 0.55802 \n",
      "CPU times: user 38min 49s, sys: 5.92 s, total: 38min 55s\n",
      "Wall time: 5min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "\n",
    "loss_list = []\n",
    "temp = train_metadata_kaggle.copy()\n",
    "\n",
    "print(temp.shape)\n",
    "temp.fillna(0, inplace=True)\n",
    "\n",
    "y = temp['target']\n",
    "del temp['target']\n",
    "classes = sorted(y.unique())\n",
    "\n",
    "# Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "# with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "class_weight = {\n",
    "    c: 1 for c in classes\n",
    "}\n",
    "for c in [64, 15]:\n",
    "    class_weight[c] = 2\n",
    "\n",
    "#print('Unique classes : ', classes)\n",
    "\n",
    "train_id = temp['object_id']\n",
    "del temp['object_id']\n",
    "# Compute weights\n",
    "w = y.value_counts()\n",
    "weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=51)\n",
    "clfs = []\n",
    "importances = pd.DataFrame()\n",
    "lgb_params = {\n",
    "'random_state':51,\n",
    "'device': 'cpu', \n",
    "'objective': 'multiclass', \n",
    "'num_class': 14, \n",
    "'boosting_type': 'gbdt', \n",
    "'n_jobs': -1, \n",
    "'n_estimators': 4000, \n",
    "'subsample_freq': 6, \n",
    "'subsample_for_bin': 5000, \n",
    "'min_data_per_group': 100, \n",
    "'max_cat_to_onehot': 4, \n",
    "'cat_l2': 1.0, \n",
    "'cat_smooth': 59.5, \n",
    "'max_cat_threshold': 32, \n",
    "'metric_freq': 10, \n",
    "'verbosity': -1, \n",
    "'metric': 'multi_logloss', \n",
    "'xgboost_dart_mode': False, \n",
    "'uniform_drop': False, \n",
    "'colsample_bytree': 0.5, \n",
    "'drop_rate': 0.173, \n",
    "'learning_rate': 0.0267, \n",
    "'max_drop': 5, \n",
    "'min_child_samples': 40,\n",
    "'min_child_weight': 150, \n",
    "'min_split_gain': 0.15, \n",
    "'num_leaves': 6, \n",
    "'reg_alpha': 0.1, \n",
    "'reg_lambda': 0.001, \n",
    "'skip_drop': 0.44, \n",
    "'subsample': 0.75}\n",
    "oof_preds = np.zeros((len(temp), np.unique(y).shape[0]))\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "    trn_x, trn_y = temp.iloc[trn_], y.iloc[trn_]\n",
    "    val_x, val_y = temp.iloc[val_], y.iloc[val_]\n",
    "\n",
    "    trn_xa, trn_y, val_xa, val_y=smoteAdataset(trn_x.values, trn_y.values, val_x.values, val_y.values)\n",
    "    trn_x=pd.DataFrame(data=trn_xa, columns=trn_x.columns)\n",
    "    val_x=pd.DataFrame(data=val_xa, columns=val_x.columns)\n",
    "    \n",
    "    print(trn_x.shape,trn_y.shape,val_x.shape,val_y.shape)\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(**lgb_params)\n",
    "    clf.fit(\n",
    "        trn_x, trn_y,\n",
    "        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "        eval_metric=lgb_multi_weighted_logloss,\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=50,\n",
    "        sample_weight=trn_y.map(weights)\n",
    "    )\n",
    "    oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "    loss_oof = multi_weighted_logloss(val_y, oof_preds[val_, :])\n",
    "    #loss_list.append(loss_oof)\n",
    "    print(fold_,loss_oof)\n",
    "\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = temp.columns\n",
    "    imp_df['gain'] = clf.feature_importances_\n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    clfs.append(clf)\n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "#final_dict[column_] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used_columns = used_columns1 + used_columns2 + used_columns3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_metadata.drop([x for x in train_metadata.columns if x not in ['object_id'] + used_columns ] ,axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_metadata = test_metadata[[x for x in test_metadata.columns if x in ['object_id'] + used_columns ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 139) (3492890, 138)\n"
     ]
    }
   ],
   "source": [
    "print(train_metadata_kaggle.shape,test_metadata_kaggle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>flux_min</th>\n",
       "      <th>flux_max</th>\n",
       "      <th>flux_mean</th>\n",
       "      <th>flux_median</th>\n",
       "      <th>flux_std</th>\n",
       "      <th>flux_skew</th>\n",
       "      <th>flux_err_min</th>\n",
       "      <th>flux_err_max</th>\n",
       "      <th>flux_err_mean</th>\n",
       "      <th>flux_err_median</th>\n",
       "      <th>flux_err_std</th>\n",
       "      <th>flux_err_skew</th>\n",
       "      <th>detected_mean</th>\n",
       "      <th>flux_ratio_sq_sum</th>\n",
       "      <th>flux_ratio_sq_skew</th>\n",
       "      <th>flux_by_flux_ratio_sq_sum</th>\n",
       "      <th>flux_by_flux_ratio_sq_skew</th>\n",
       "      <th>flux_w_mean</th>\n",
       "      <th>flux_diff1</th>\n",
       "      <th>flux_diff2</th>\n",
       "      <th>flux_diff3</th>\n",
       "      <th>0__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>0__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>0__kurtosis</th>\n",
       "      <th>0__skewness</th>\n",
       "      <th>1__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>1__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>1__kurtosis</th>\n",
       "      <th>1__skewness</th>\n",
       "      <th>2__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>2__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>2__kurtosis</th>\n",
       "      <th>2__skewness</th>\n",
       "      <th>3__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>3__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>3__kurtosis</th>\n",
       "      <th>3__skewness</th>\n",
       "      <th>4__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>4__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>4__kurtosis</th>\n",
       "      <th>4__skewness</th>\n",
       "      <th>5__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>5__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>5__kurtosis</th>\n",
       "      <th>5__skewness</th>\n",
       "      <th>flux__length</th>\n",
       "      <th>flux__longest_strike_above_mean</th>\n",
       "      <th>flux__longest_strike_below_mean</th>\n",
       "      <th>flux__mean_abs_change</th>\n",
       "      <th>flux__mean_change</th>\n",
       "      <th>flux_by_flux_ratio_sq__longest_strike_above_mean</th>\n",
       "      <th>flux_by_flux_ratio_sq__longest_strike_below_mean</th>\n",
       "      <th>mjd__mean_abs_change</th>\n",
       "      <th>mjd__mean_change</th>\n",
       "      <th>mjd_diff_det</th>\n",
       "      <th>hostgal_photoz</th>\n",
       "      <th>hostgal_photoz_err</th>\n",
       "      <th>distmod</th>\n",
       "      <th>mwebv</th>\n",
       "      <th>target</th>\n",
       "      <th>haversine</th>\n",
       "      <th>latlon1</th>\n",
       "      <th>hostgal_photoz_certain</th>\n",
       "      <th>A0_sum_flux</th>\n",
       "      <th>A0_mean_flux</th>\n",
       "      <th>A0_std_detected</th>\n",
       "      <th>A1_mean_detected</th>\n",
       "      <th>A2_sum_detected</th>\n",
       "      <th>A4_mean_detected</th>\n",
       "      <th>A5_std_detected</th>\n",
       "      <th>A5_mean_detected</th>\n",
       "      <th>percent_p2_region_minus_1</th>\n",
       "      <th>A2_min_flux</th>\n",
       "      <th>A5_sum_detected</th>\n",
       "      <th>__flux_percentile_ratio_mid50___5_</th>\n",
       "      <th>__flux_percentile_ratio_mid65___2_</th>\n",
       "      <th>__median_absolute_deviation___2_</th>\n",
       "      <th>__qso_log_chi2_qsonu___0_</th>\n",
       "      <th>__stetson_k___1_</th>\n",
       "      <th>__freq1_signif___2_</th>\n",
       "      <th>__stetson_k___2_</th>\n",
       "      <th>__freq3_amplitude1___1_</th>\n",
       "      <th>__median_absolute_deviation___2_.1</th>\n",
       "      <th>__percent_close_to_median___2_</th>\n",
       "      <th>__freq_varrat___5_</th>\n",
       "      <th>__freq_varrat___4_</th>\n",
       "      <th>__qso_log_chi2_qsonu___3_</th>\n",
       "      <th>__qso_log_chi2_qsonu___1_</th>\n",
       "      <th>__qso_log_chi2_qsonu___5_</th>\n",
       "      <th>__std___4_</th>\n",
       "      <th>__freq_varrat___3_</th>\n",
       "      <th>__amplitude___2_</th>\n",
       "      <th>outlierScore</th>\n",
       "      <th>hipd</th>\n",
       "      <th>lipd</th>\n",
       "      <th>highEnergy_transitory_1.0_TF</th>\n",
       "      <th>highEnergy_transitory_1.5_TF</th>\n",
       "      <th>lowEnergy_transitory_1.0_TF</th>\n",
       "      <th>lowEnergy_transitory_1.5_TF</th>\n",
       "      <th>A1_minus_3_sigma</th>\n",
       "      <th>A5_max_median_diff_flux</th>\n",
       "      <th>A5_minus_3_sigma</th>\n",
       "      <th>A5_max_mean_diff_flux</th>\n",
       "      <th>diff_A5_A4_max_min_flux</th>\n",
       "      <th>diff_A2_A1_max_min_flux</th>\n",
       "      <th>diff_A3_A2_median_min_flux</th>\n",
       "      <th>diff_A5_A4_max_median_flux</th>\n",
       "      <th>diff_A4_A3_max_median_flux</th>\n",
       "      <th>diff_A2_A0_median_min_flux</th>\n",
       "      <th>diff_A4_A3_max_mean_flux</th>\n",
       "      <th>diff_A5_A2_max_mean_flux</th>\n",
       "      <th>diff_A5_A3_max_mean_flux</th>\n",
       "      <th>diff_A4_A0_median_mean_flux</th>\n",
       "      <th>diff_A5_A4_max_mean_flux</th>\n",
       "      <th>diff_A2_A1_max_median_flux</th>\n",
       "      <th>diff_A5_A2_max_median_flux</th>\n",
       "      <th>diff_A5_A4_median_min_flux</th>\n",
       "      <th>diff_A4_A0_median_min_flux</th>\n",
       "      <th>diff_A4_A1_max_median_flux</th>\n",
       "      <th>diff_A4_A2_max_median_flux</th>\n",
       "      <th>diff_A5_A4_minus_1_sigma</th>\n",
       "      <th>diff_A5_A3_median_min_flux</th>\n",
       "      <th>diff_A5_A3_max_median_flux</th>\n",
       "      <th>diff_A3_A1_minus_1_sigma</th>\n",
       "      <th>diff_A3_A0_median_min_flux</th>\n",
       "      <th>diff_A3_A0_plus_1_sigma</th>\n",
       "      <th>diff_A1_A0_median_min_flux</th>\n",
       "      <th>diff_A4_A2_mean_min_flux</th>\n",
       "      <th>diff_A5_A1_plus_1_sigma</th>\n",
       "      <th>diff_A4_A1_median_mean_flux</th>\n",
       "      <th>diff_A3_A2_max_median_flux</th>\n",
       "      <th>diff_A5_A1_median_mean_flux</th>\n",
       "      <th>div_A4_A2_median_min_flux</th>\n",
       "      <th>div_A5_A2_median_min_flux</th>\n",
       "      <th>div_A5_A2_minus_1_sigma</th>\n",
       "      <th>div_A5_A4_median_mean_flux</th>\n",
       "      <th>div_A3_A0_plus_1_sigma</th>\n",
       "      <th>div_A4_A1_minus_1_sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>-1100.440063</td>\n",
       "      <td>660.626343</td>\n",
       "      <td>-123.096998</td>\n",
       "      <td>-89.477524</td>\n",
       "      <td>394.109851</td>\n",
       "      <td>-0.349540</td>\n",
       "      <td>2.130510</td>\n",
       "      <td>12.845472</td>\n",
       "      <td>4.482743</td>\n",
       "      <td>3.835269</td>\n",
       "      <td>1.744747</td>\n",
       "      <td>1.623740</td>\n",
       "      <td>0.946023</td>\n",
       "      <td>2.929669e+06</td>\n",
       "      <td>0.812722</td>\n",
       "      <td>-9.601766e+08</td>\n",
       "      <td>-1.414322</td>\n",
       "      <td>-327.742307</td>\n",
       "      <td>1761.066406</td>\n",
       "      <td>-14.306331</td>\n",
       "      <td>-5.373326</td>\n",
       "      <td>205.036926</td>\n",
       "      <td>1628.427737</td>\n",
       "      <td>-1.475181</td>\n",
       "      <td>0.128917</td>\n",
       "      <td>22370.594834</td>\n",
       "      <td>2806.374162</td>\n",
       "      <td>-1.255123</td>\n",
       "      <td>0.415580</td>\n",
       "      <td>7780.500807</td>\n",
       "      <td>2805.598113</td>\n",
       "      <td>-1.409885</td>\n",
       "      <td>0.339918</td>\n",
       "      <td>7024.003068</td>\n",
       "      <td>2536.068846</td>\n",
       "      <td>-1.449858</td>\n",
       "      <td>0.293128</td>\n",
       "      <td>3245.366349</td>\n",
       "      <td>2741.539785</td>\n",
       "      <td>-1.548319</td>\n",
       "      <td>0.200096</td>\n",
       "      <td>2704.641265</td>\n",
       "      <td>2893.344217</td>\n",
       "      <td>-1.592820</td>\n",
       "      <td>0.125268</td>\n",
       "      <td>352.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>202.114067</td>\n",
       "      <td>1.999688</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.631898</td>\n",
       "      <td>2.631898</td>\n",
       "      <td>873.7903</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>92</td>\n",
       "      <td>0.319006</td>\n",
       "      <td>-1.528827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-205.03693</td>\n",
       "      <td>-3.254554</td>\n",
       "      <td>0.3528</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>57</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.2854</td>\n",
       "      <td>0.9120</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-682.000</td>\n",
       "      <td>52</td>\n",
       "      <td>5.562230e-26</td>\n",
       "      <td>6.719410e-20</td>\n",
       "      <td>368.12900</td>\n",
       "      <td>6.217890</td>\n",
       "      <td>1.091730</td>\n",
       "      <td>5.49891</td>\n",
       "      <td>1.053490</td>\n",
       "      <td>114.465000</td>\n",
       "      <td>368.12900</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.401664</td>\n",
       "      <td>0.129578</td>\n",
       "      <td>9.16612</td>\n",
       "      <td>9.508750</td>\n",
       "      <td>7.344980</td>\n",
       "      <td>289.27700</td>\n",
       "      <td>0.110785</td>\n",
       "      <td>646.9220</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2191.161900</td>\n",
       "      <td>463.712450</td>\n",
       "      <td>-931.788350</td>\n",
       "      <td>425.637990</td>\n",
       "      <td>-3.135000</td>\n",
       "      <td>-466.641780</td>\n",
       "      <td>-47.8000</td>\n",
       "      <td>-21.782660</td>\n",
       "      <td>-122.341950</td>\n",
       "      <td>309.302005</td>\n",
       "      <td>-128.932226</td>\n",
       "      <td>-320.493130</td>\n",
       "      <td>-141.202570</td>\n",
       "      <td>-40.826106</td>\n",
       "      <td>-12.270344</td>\n",
       "      <td>-270.841780</td>\n",
       "      <td>-414.072110</td>\n",
       "      <td>18.647660</td>\n",
       "      <td>211.745135</td>\n",
       "      <td>-663.131230</td>\n",
       "      <td>-392.289450</td>\n",
       "      <td>5.528684</td>\n",
       "      <td>-31.109210</td>\n",
       "      <td>-144.124610</td>\n",
       "      <td>531.058750</td>\n",
       "      <td>261.502005</td>\n",
       "      <td>133.631375</td>\n",
       "      <td>505.102005</td>\n",
       "      <td>-181.623534</td>\n",
       "      <td>31.342350</td>\n",
       "      <td>54.613224</td>\n",
       "      <td>-269.947500</td>\n",
       "      <td>64.125540</td>\n",
       "      <td>0.765601</td>\n",
       "      <td>0.810406</td>\n",
       "      <td>0.580770</td>\n",
       "      <td>0.800106</td>\n",
       "      <td>2.656105</td>\n",
       "      <td>0.352129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>713</td>\n",
       "      <td>-14.735178</td>\n",
       "      <td>14.770886</td>\n",
       "      <td>-1.423351</td>\n",
       "      <td>-0.873033</td>\n",
       "      <td>6.471144</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.639458</td>\n",
       "      <td>9.115748</td>\n",
       "      <td>2.359620</td>\n",
       "      <td>1.998217</td>\n",
       "      <td>1.509888</td>\n",
       "      <td>1.633246</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>5.886068e+03</td>\n",
       "      <td>3.439423</td>\n",
       "      <td>-2.875087e+04</td>\n",
       "      <td>-3.454554</td>\n",
       "      <td>-4.884564</td>\n",
       "      <td>29.506064</td>\n",
       "      <td>-20.730002</td>\n",
       "      <td>-6.040676</td>\n",
       "      <td>190.427851</td>\n",
       "      <td>299.586559</td>\n",
       "      <td>-1.014003</td>\n",
       "      <td>0.260052</td>\n",
       "      <td>57.109047</td>\n",
       "      <td>192.539229</td>\n",
       "      <td>-1.097170</td>\n",
       "      <td>-0.087865</td>\n",
       "      <td>44.477327</td>\n",
       "      <td>191.057528</td>\n",
       "      <td>-1.188472</td>\n",
       "      <td>-0.022678</td>\n",
       "      <td>55.270113</td>\n",
       "      <td>212.522263</td>\n",
       "      <td>-1.142896</td>\n",
       "      <td>-0.167176</td>\n",
       "      <td>50.414646</td>\n",
       "      <td>203.892482</td>\n",
       "      <td>-1.190245</td>\n",
       "      <td>-0.064134</td>\n",
       "      <td>100.473776</td>\n",
       "      <td>143.963093</td>\n",
       "      <td>-0.797047</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>350.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.935177</td>\n",
       "      <td>-0.050944</td>\n",
       "      <td>199.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.352571</td>\n",
       "      <td>14.352571</td>\n",
       "      <td>846.8017</td>\n",
       "      <td>1.6267</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>45.4063</td>\n",
       "      <td>0.007</td>\n",
       "      <td>88</td>\n",
       "      <td>1.698939</td>\n",
       "      <td>3.258921</td>\n",
       "      <td>2.099614</td>\n",
       "      <td>-190.42786</td>\n",
       "      <td>-2.720398</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-10.070</td>\n",
       "      <td>0</td>\n",
       "      <td>2.119070e-02</td>\n",
       "      <td>8.243180e-02</td>\n",
       "      <td>5.10035</td>\n",
       "      <td>2.187190</td>\n",
       "      <td>1.066100</td>\n",
       "      <td>3.95669</td>\n",
       "      <td>1.088180</td>\n",
       "      <td>0.851103</td>\n",
       "      <td>5.10035</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.369518</td>\n",
       "      <td>0.166179</td>\n",
       "      <td>2.79753</td>\n",
       "      <td>3.124810</td>\n",
       "      <td>0.659762</td>\n",
       "      <td>6.34953</td>\n",
       "      <td>0.111883</td>\n",
       "      <td>10.2985</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.909016</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-18.157002</td>\n",
       "      <td>17.233897</td>\n",
       "      <td>-23.076394</td>\n",
       "      <td>16.565061</td>\n",
       "      <td>6.867315</td>\n",
       "      <td>-0.249980</td>\n",
       "      <td>2.3741</td>\n",
       "      <td>6.613788</td>\n",
       "      <td>-0.784106</td>\n",
       "      <td>-1.686373</td>\n",
       "      <td>-1.589087</td>\n",
       "      <td>5.241782</td>\n",
       "      <td>4.247778</td>\n",
       "      <td>0.484493</td>\n",
       "      <td>5.836865</td>\n",
       "      <td>0.956520</td>\n",
       "      <td>6.586856</td>\n",
       "      <td>0.253527</td>\n",
       "      <td>-0.143748</td>\n",
       "      <td>0.929589</td>\n",
       "      <td>-0.026931</td>\n",
       "      <td>-1.580997</td>\n",
       "      <td>-0.577947</td>\n",
       "      <td>5.829681</td>\n",
       "      <td>-0.705045</td>\n",
       "      <td>0.687727</td>\n",
       "      <td>1.070336</td>\n",
       "      <td>-0.479873</td>\n",
       "      <td>2.110778</td>\n",
       "      <td>0.607564</td>\n",
       "      <td>-0.350414</td>\n",
       "      <td>0.757175</td>\n",
       "      <td>-1.127337</td>\n",
       "      <td>1.155007</td>\n",
       "      <td>1.180482</td>\n",
       "      <td>1.353889</td>\n",
       "      <td>-6.188008</td>\n",
       "      <td>1.243640</td>\n",
       "      <td>1.085396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>730</td>\n",
       "      <td>-19.159811</td>\n",
       "      <td>47.310059</td>\n",
       "      <td>2.267434</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>8.022239</td>\n",
       "      <td>3.177854</td>\n",
       "      <td>0.695106</td>\n",
       "      <td>11.281384</td>\n",
       "      <td>2.471061</td>\n",
       "      <td>1.990851</td>\n",
       "      <td>1.721134</td>\n",
       "      <td>1.823726</td>\n",
       "      <td>0.069697</td>\n",
       "      <td>4.124452e+03</td>\n",
       "      <td>5.480405</td>\n",
       "      <td>1.046502e+05</td>\n",
       "      <td>5.989138</td>\n",
       "      <td>25.373110</td>\n",
       "      <td>66.469870</td>\n",
       "      <td>29.315018</td>\n",
       "      <td>2.619697</td>\n",
       "      <td>3.461790</td>\n",
       "      <td>4.729538</td>\n",
       "      <td>0.474215</td>\n",
       "      <td>0.356910</td>\n",
       "      <td>7.334944</td>\n",
       "      <td>13.515895</td>\n",
       "      <td>0.976374</td>\n",
       "      <td>0.471342</td>\n",
       "      <td>124.845250</td>\n",
       "      <td>119.500254</td>\n",
       "      <td>5.131290</td>\n",
       "      <td>2.385066</td>\n",
       "      <td>168.280524</td>\n",
       "      <td>162.799417</td>\n",
       "      <td>7.125665</td>\n",
       "      <td>2.662075</td>\n",
       "      <td>219.745132</td>\n",
       "      <td>202.532898</td>\n",
       "      <td>6.081065</td>\n",
       "      <td>2.537802</td>\n",
       "      <td>231.509177</td>\n",
       "      <td>199.286370</td>\n",
       "      <td>3.583130</td>\n",
       "      <td>1.680352</td>\n",
       "      <td>330.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.227614</td>\n",
       "      <td>-0.008131</td>\n",
       "      <td>4.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>3.580623</td>\n",
       "      <td>3.580623</td>\n",
       "      <td>78.7737</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>40.2561</td>\n",
       "      <td>0.021</td>\n",
       "      <td>42</td>\n",
       "      <td>1.818030</td>\n",
       "      <td>3.128522</td>\n",
       "      <td>0.229779</td>\n",
       "      <td>-3.46179</td>\n",
       "      <td>-0.048080</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0980</td>\n",
       "      <td>0.2715</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>0.769</td>\n",
       "      <td>-2.850</td>\n",
       "      <td>4</td>\n",
       "      <td>1.942280e-04</td>\n",
       "      <td>5.511800e-01</td>\n",
       "      <td>1.04253</td>\n",
       "      <td>-0.307228</td>\n",
       "      <td>0.933091</td>\n",
       "      <td>4.61663</td>\n",
       "      <td>0.634723</td>\n",
       "      <td>0.454918</td>\n",
       "      <td>1.04253</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.500549</td>\n",
       "      <td>0.318256</td>\n",
       "      <td>3.04833</td>\n",
       "      <td>0.127758</td>\n",
       "      <td>1.669430</td>\n",
       "      <td>10.60480</td>\n",
       "      <td>0.292954</td>\n",
       "      <td>11.9218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.280586</td>\n",
       "      <td>44.767413</td>\n",
       "      <td>-35.458878</td>\n",
       "      <td>42.770664</td>\n",
       "      <td>19.473581</td>\n",
       "      <td>14.758601</td>\n",
       "      <td>2.7569</td>\n",
       "      <td>4.611787</td>\n",
       "      <td>7.244126</td>\n",
       "      <td>-0.142353</td>\n",
       "      <td>6.515316</td>\n",
       "      <td>24.176824</td>\n",
       "      <td>12.434728</td>\n",
       "      <td>-3.376547</td>\n",
       "      <td>5.919412</td>\n",
       "      <td>14.981301</td>\n",
       "      <td>24.264403</td>\n",
       "      <td>14.861794</td>\n",
       "      <td>3.356611</td>\n",
       "      <td>34.633917</td>\n",
       "      <td>19.652616</td>\n",
       "      <td>-2.391746</td>\n",
       "      <td>15.603858</td>\n",
       "      <td>11.855913</td>\n",
       "      <td>-3.289694</td>\n",
       "      <td>2.614547</td>\n",
       "      <td>9.647359</td>\n",
       "      <td>0.080347</td>\n",
       "      <td>4.894168</td>\n",
       "      <td>15.923825</td>\n",
       "      <td>-3.334674</td>\n",
       "      <td>12.408490</td>\n",
       "      <td>-2.027049</td>\n",
       "      <td>2.047061</td>\n",
       "      <td>6.494436</td>\n",
       "      <td>2.783931</td>\n",
       "      <td>0.604275</td>\n",
       "      <td>6.417455</td>\n",
       "      <td>3.842210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>745</td>\n",
       "      <td>-15.494463</td>\n",
       "      <td>220.795212</td>\n",
       "      <td>8.909206</td>\n",
       "      <td>1.035895</td>\n",
       "      <td>27.558208</td>\n",
       "      <td>4.979826</td>\n",
       "      <td>0.567170</td>\n",
       "      <td>55.892746</td>\n",
       "      <td>2.555576</td>\n",
       "      <td>1.819875</td>\n",
       "      <td>3.537324</td>\n",
       "      <td>10.741655</td>\n",
       "      <td>0.173789</td>\n",
       "      <td>9.416165e+04</td>\n",
       "      <td>9.611274</td>\n",
       "      <td>1.439125e+07</td>\n",
       "      <td>11.141069</td>\n",
       "      <td>152.835617</td>\n",
       "      <td>236.289675</td>\n",
       "      <td>26.521968</td>\n",
       "      <td>1.546038</td>\n",
       "      <td>129.421659</td>\n",
       "      <td>123.298327</td>\n",
       "      <td>4.629801</td>\n",
       "      <td>2.023211</td>\n",
       "      <td>320.174052</td>\n",
       "      <td>280.440312</td>\n",
       "      <td>50.868880</td>\n",
       "      <td>7.007099</td>\n",
       "      <td>543.845781</td>\n",
       "      <td>491.548270</td>\n",
       "      <td>36.088137</td>\n",
       "      <td>5.688194</td>\n",
       "      <td>807.123762</td>\n",
       "      <td>710.721942</td>\n",
       "      <td>16.392533</td>\n",
       "      <td>3.751603</td>\n",
       "      <td>735.528417</td>\n",
       "      <td>680.055280</td>\n",
       "      <td>13.747434</td>\n",
       "      <td>3.476420</td>\n",
       "      <td>591.037583</td>\n",
       "      <td>523.503586</td>\n",
       "      <td>12.134629</td>\n",
       "      <td>3.170857</td>\n",
       "      <td>351.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>7.065548</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>4.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2.061453</td>\n",
       "      <td>2.061453</td>\n",
       "      <td>123.6872</td>\n",
       "      <td>0.2813</td>\n",
       "      <td>1.1523</td>\n",
       "      <td>40.7951</td>\n",
       "      <td>0.007</td>\n",
       "      <td>90</td>\n",
       "      <td>0.495223</td>\n",
       "      <td>6.893743</td>\n",
       "      <td>0.890445</td>\n",
       "      <td>129.42166</td>\n",
       "      <td>1.797523</td>\n",
       "      <td>0.1655</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2322</td>\n",
       "      <td>0.3364</td>\n",
       "      <td>0.1273</td>\n",
       "      <td>0.768</td>\n",
       "      <td>-2.160</td>\n",
       "      <td>7</td>\n",
       "      <td>8.401600e-03</td>\n",
       "      <td>5.463690e-01</td>\n",
       "      <td>1.41645</td>\n",
       "      <td>1.432200</td>\n",
       "      <td>0.295163</td>\n",
       "      <td>3.96789</td>\n",
       "      <td>0.394683</td>\n",
       "      <td>3.595670</td>\n",
       "      <td>1.41645</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.489589</td>\n",
       "      <td>0.360868</td>\n",
       "      <td>6.06886</td>\n",
       "      <td>5.840820</td>\n",
       "      <td>2.820440</td>\n",
       "      <td>32.77250</td>\n",
       "      <td>0.290652</td>\n",
       "      <td>111.4770</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.173977</td>\n",
       "      <td>138.763735</td>\n",
       "      <td>-67.434252</td>\n",
       "      <td>130.767152</td>\n",
       "      <td>-47.364906</td>\n",
       "      <td>27.090910</td>\n",
       "      <td>3.7217</td>\n",
       "      <td>-43.599234</td>\n",
       "      <td>-19.526731</td>\n",
       "      <td>-2.346763</td>\n",
       "      <td>-18.339091</td>\n",
       "      <td>-80.316526</td>\n",
       "      <td>-58.070623</td>\n",
       "      <td>-11.123477</td>\n",
       "      <td>-39.731532</td>\n",
       "      <td>29.014610</td>\n",
       "      <td>-81.607175</td>\n",
       "      <td>-3.765671</td>\n",
       "      <td>11.833550</td>\n",
       "      <td>-8.993331</td>\n",
       "      <td>-38.007941</td>\n",
       "      <td>4.620626</td>\n",
       "      <td>6.692942</td>\n",
       "      <td>-63.125965</td>\n",
       "      <td>-0.310113</td>\n",
       "      <td>1.374937</td>\n",
       "      <td>43.208654</td>\n",
       "      <td>-0.423063</td>\n",
       "      <td>16.757367</td>\n",
       "      <td>5.121609</td>\n",
       "      <td>-7.032285</td>\n",
       "      <td>-18.481210</td>\n",
       "      <td>-3.164583</td>\n",
       "      <td>6.487100</td>\n",
       "      <td>5.029966</td>\n",
       "      <td>0.688379</td>\n",
       "      <td>0.674005</td>\n",
       "      <td>8.000790</td>\n",
       "      <td>0.984685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1124</td>\n",
       "      <td>-16.543753</td>\n",
       "      <td>143.600189</td>\n",
       "      <td>7.145702</td>\n",
       "      <td>1.141288</td>\n",
       "      <td>20.051722</td>\n",
       "      <td>4.406298</td>\n",
       "      <td>0.695277</td>\n",
       "      <td>11.383690</td>\n",
       "      <td>2.753004</td>\n",
       "      <td>2.214854</td>\n",
       "      <td>1.933837</td>\n",
       "      <td>1.794938</td>\n",
       "      <td>0.173295</td>\n",
       "      <td>3.432418e+04</td>\n",
       "      <td>7.868462</td>\n",
       "      <td>3.015599e+06</td>\n",
       "      <td>7.908174</td>\n",
       "      <td>87.856390</td>\n",
       "      <td>160.143942</td>\n",
       "      <td>22.411225</td>\n",
       "      <td>1.822792</td>\n",
       "      <td>41.639721</td>\n",
       "      <td>32.987125</td>\n",
       "      <td>0.822496</td>\n",
       "      <td>-0.332169</td>\n",
       "      <td>268.808929</td>\n",
       "      <td>207.812015</td>\n",
       "      <td>6.112295</td>\n",
       "      <td>2.377222</td>\n",
       "      <td>594.150153</td>\n",
       "      <td>498.509820</td>\n",
       "      <td>10.343254</td>\n",
       "      <td>3.075437</td>\n",
       "      <td>643.020183</td>\n",
       "      <td>555.512641</td>\n",
       "      <td>14.095862</td>\n",
       "      <td>3.603208</td>\n",
       "      <td>574.553907</td>\n",
       "      <td>524.107264</td>\n",
       "      <td>16.377058</td>\n",
       "      <td>3.904008</td>\n",
       "      <td>393.114268</td>\n",
       "      <td>357.907185</td>\n",
       "      <td>14.434470</td>\n",
       "      <td>3.657305</td>\n",
       "      <td>352.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>6.727352</td>\n",
       "      <td>0.012543</td>\n",
       "      <td>10.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>2.231855</td>\n",
       "      <td>2.231855</td>\n",
       "      <td>133.9113</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>40.4166</td>\n",
       "      <td>0.024</td>\n",
       "      <td>90</td>\n",
       "      <td>0.395162</td>\n",
       "      <td>-1.928064</td>\n",
       "      <td>0.245788</td>\n",
       "      <td>41.63972</td>\n",
       "      <td>0.660948</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1724</td>\n",
       "      <td>0.2578</td>\n",
       "      <td>0.0702</td>\n",
       "      <td>0.707</td>\n",
       "      <td>-2.084</td>\n",
       "      <td>4</td>\n",
       "      <td>2.702260e-02</td>\n",
       "      <td>6.018110e-01</td>\n",
       "      <td>1.33779</td>\n",
       "      <td>-0.064359</td>\n",
       "      <td>0.674119</td>\n",
       "      <td>5.24444</td>\n",
       "      <td>0.560453</td>\n",
       "      <td>1.205580</td>\n",
       "      <td>1.33779</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.673592</td>\n",
       "      <td>0.382847</td>\n",
       "      <td>5.07231</td>\n",
       "      <td>3.407900</td>\n",
       "      <td>2.312920</td>\n",
       "      <td>26.63330</td>\n",
       "      <td>0.250639</td>\n",
       "      <td>54.3781</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-19.689575</td>\n",
       "      <td>107.184313</td>\n",
       "      <td>-57.407143</td>\n",
       "      <td>102.260844</td>\n",
       "      <td>-40.125818</td>\n",
       "      <td>68.962513</td>\n",
       "      <td>0.8410</td>\n",
       "      <td>-34.670865</td>\n",
       "      <td>3.050778</td>\n",
       "      <td>-4.412730</td>\n",
       "      <td>4.962243</td>\n",
       "      <td>5.833122</td>\n",
       "      <td>-26.471001</td>\n",
       "      <td>-8.081169</td>\n",
       "      <td>-31.433244</td>\n",
       "      <td>69.766513</td>\n",
       "      <td>1.401623</td>\n",
       "      <td>-5.454953</td>\n",
       "      <td>10.903035</td>\n",
       "      <td>105.839001</td>\n",
       "      <td>36.072488</td>\n",
       "      <td>2.421924</td>\n",
       "      <td>9.019812</td>\n",
       "      <td>-31.620087</td>\n",
       "      <td>-11.709569</td>\n",
       "      <td>-3.571730</td>\n",
       "      <td>34.336172</td>\n",
       "      <td>-3.608730</td>\n",
       "      <td>14.121887</td>\n",
       "      <td>15.590844</td>\n",
       "      <td>-4.682090</td>\n",
       "      <td>33.021710</td>\n",
       "      <td>-1.444469</td>\n",
       "      <td>6.151620</td>\n",
       "      <td>4.316788</td>\n",
       "      <td>1.312571</td>\n",
       "      <td>0.603286</td>\n",
       "      <td>12.365708</td>\n",
       "      <td>4.881188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id     flux_min    flux_max   flux_mean  flux_median    flux_std  \\\n",
       "0        615 -1100.440063  660.626343 -123.096998   -89.477524  394.109851   \n",
       "1        713   -14.735178   14.770886   -1.423351    -0.873033    6.471144   \n",
       "2        730   -19.159811   47.310059    2.267434     0.409172    8.022239   \n",
       "3        745   -15.494463  220.795212    8.909206     1.035895   27.558208   \n",
       "4       1124   -16.543753  143.600189    7.145702     1.141288   20.051722   \n",
       "\n",
       "   flux_skew  flux_err_min  flux_err_max  flux_err_mean  flux_err_median  \\\n",
       "0  -0.349540      2.130510     12.845472       4.482743         3.835269   \n",
       "1   0.014989      0.639458      9.115748       2.359620         1.998217   \n",
       "2   3.177854      0.695106     11.281384       2.471061         1.990851   \n",
       "3   4.979826      0.567170     55.892746       2.555576         1.819875   \n",
       "4   4.406298      0.695277     11.383690       2.753004         2.214854   \n",
       "\n",
       "   flux_err_std  flux_err_skew  detected_mean  flux_ratio_sq_sum  \\\n",
       "0      1.744747       1.623740       0.946023       2.929669e+06   \n",
       "1      1.509888       1.633246       0.171429       5.886068e+03   \n",
       "2      1.721134       1.823726       0.069697       4.124452e+03   \n",
       "3      3.537324      10.741655       0.173789       9.416165e+04   \n",
       "4      1.933837       1.794938       0.173295       3.432418e+04   \n",
       "\n",
       "   flux_ratio_sq_skew  flux_by_flux_ratio_sq_sum  flux_by_flux_ratio_sq_skew  \\\n",
       "0            0.812722              -9.601766e+08                   -1.414322   \n",
       "1            3.439423              -2.875087e+04                   -3.454554   \n",
       "2            5.480405               1.046502e+05                    5.989138   \n",
       "3            9.611274               1.439125e+07                   11.141069   \n",
       "4            7.868462               3.015599e+06                    7.908174   \n",
       "\n",
       "   flux_w_mean   flux_diff1  flux_diff2  flux_diff3  \\\n",
       "0  -327.742307  1761.066406  -14.306331   -5.373326   \n",
       "1    -4.884564    29.506064  -20.730002   -6.040676   \n",
       "2    25.373110    66.469870   29.315018    2.619697   \n",
       "3   152.835617   236.289675   26.521968    1.546038   \n",
       "4    87.856390   160.143942   22.411225    1.822792   \n",
       "\n",
       "   0__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                               205.036926   \n",
       "1                               190.427851   \n",
       "2                                 3.461790   \n",
       "3                               129.421659   \n",
       "4                                41.639721   \n",
       "\n",
       "   0__fft_coefficient__coeff_1__attr_\"abs\"  0__kurtosis  0__skewness  \\\n",
       "0                              1628.427737    -1.475181     0.128917   \n",
       "1                               299.586559    -1.014003     0.260052   \n",
       "2                                 4.729538     0.474215     0.356910   \n",
       "3                               123.298327     4.629801     2.023211   \n",
       "4                                32.987125     0.822496    -0.332169   \n",
       "\n",
       "   1__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                             22370.594834   \n",
       "1                                57.109047   \n",
       "2                                 7.334944   \n",
       "3                               320.174052   \n",
       "4                               268.808929   \n",
       "\n",
       "   1__fft_coefficient__coeff_1__attr_\"abs\"  1__kurtosis  1__skewness  \\\n",
       "0                              2806.374162    -1.255123     0.415580   \n",
       "1                               192.539229    -1.097170    -0.087865   \n",
       "2                                13.515895     0.976374     0.471342   \n",
       "3                               280.440312    50.868880     7.007099   \n",
       "4                               207.812015     6.112295     2.377222   \n",
       "\n",
       "   2__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                              7780.500807   \n",
       "1                                44.477327   \n",
       "2                               124.845250   \n",
       "3                               543.845781   \n",
       "4                               594.150153   \n",
       "\n",
       "   2__fft_coefficient__coeff_1__attr_\"abs\"  2__kurtosis  2__skewness  \\\n",
       "0                              2805.598113    -1.409885     0.339918   \n",
       "1                               191.057528    -1.188472    -0.022678   \n",
       "2                               119.500254     5.131290     2.385066   \n",
       "3                               491.548270    36.088137     5.688194   \n",
       "4                               498.509820    10.343254     3.075437   \n",
       "\n",
       "   3__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                              7024.003068   \n",
       "1                                55.270113   \n",
       "2                               168.280524   \n",
       "3                               807.123762   \n",
       "4                               643.020183   \n",
       "\n",
       "   3__fft_coefficient__coeff_1__attr_\"abs\"  3__kurtosis  3__skewness  \\\n",
       "0                              2536.068846    -1.449858     0.293128   \n",
       "1                               212.522263    -1.142896    -0.167176   \n",
       "2                               162.799417     7.125665     2.662075   \n",
       "3                               710.721942    16.392533     3.751603   \n",
       "4                               555.512641    14.095862     3.603208   \n",
       "\n",
       "   4__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                              3245.366349   \n",
       "1                                50.414646   \n",
       "2                               219.745132   \n",
       "3                               735.528417   \n",
       "4                               574.553907   \n",
       "\n",
       "   4__fft_coefficient__coeff_1__attr_\"abs\"  4__kurtosis  4__skewness  \\\n",
       "0                              2741.539785    -1.548319     0.200096   \n",
       "1                               203.892482    -1.190245    -0.064134   \n",
       "2                               202.532898     6.081065     2.537802   \n",
       "3                               680.055280    13.747434     3.476420   \n",
       "4                               524.107264    16.377058     3.904008   \n",
       "\n",
       "   5__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                              2704.641265   \n",
       "1                               100.473776   \n",
       "2                               231.509177   \n",
       "3                               591.037583   \n",
       "4                               393.114268   \n",
       "\n",
       "   5__fft_coefficient__coeff_1__attr_\"abs\"  5__kurtosis  5__skewness  \\\n",
       "0                              2893.344217    -1.592820     0.125268   \n",
       "1                               143.963093    -0.797047     0.218182   \n",
       "2                               199.286370     3.583130     1.680352   \n",
       "3                               523.503586    12.134629     3.170857   \n",
       "4                               357.907185    14.434470     3.657305   \n",
       "\n",
       "   flux__length  flux__longest_strike_above_mean  \\\n",
       "0         352.0                             19.0   \n",
       "1         350.0                             50.0   \n",
       "2         330.0                             13.0   \n",
       "3         351.0                             19.0   \n",
       "4         352.0                             19.0   \n",
       "\n",
       "   flux__longest_strike_below_mean  flux__mean_abs_change  flux__mean_change  \\\n",
       "0                             29.0             202.114067           1.999688   \n",
       "1                             73.0               2.935177          -0.050944   \n",
       "2                             32.0               4.227614          -0.008131   \n",
       "3                            115.0               7.065548           0.008044   \n",
       "4                            158.0               6.727352           0.012543   \n",
       "\n",
       "   flux_by_flux_ratio_sq__longest_strike_above_mean  \\\n",
       "0                                              35.0   \n",
       "1                                             199.0   \n",
       "2                                               4.0   \n",
       "3                                               4.0   \n",
       "4                                              10.0   \n",
       "\n",
       "   flux_by_flux_ratio_sq__longest_strike_below_mean  mjd__mean_abs_change  \\\n",
       "0                                               4.0              2.631898   \n",
       "1                                               8.0             14.352571   \n",
       "2                                             222.0              3.580623   \n",
       "3                                             201.0              2.061453   \n",
       "4                                             231.0              2.231855   \n",
       "\n",
       "   mjd__mean_change  mjd_diff_det  hostgal_photoz  hostgal_photoz_err  \\\n",
       "0          2.631898      873.7903          0.0000              0.0000   \n",
       "1         14.352571      846.8017          1.6267              0.2552   \n",
       "2          3.580623       78.7737          0.2262              0.0157   \n",
       "3          2.061453      123.6872          0.2813              1.1523   \n",
       "4          2.231855      133.9113          0.2415              0.0176   \n",
       "\n",
       "   distmod  mwebv  target  haversine   latlon1  hostgal_photoz_certain  \\\n",
       "0      NaN  0.017      92   0.319006 -1.528827                0.000000   \n",
       "1  45.4063  0.007      88   1.698939  3.258921                2.099614   \n",
       "2  40.2561  0.021      42   1.818030  3.128522                0.229779   \n",
       "3  40.7951  0.007      90   0.495223  6.893743                0.890445   \n",
       "4  40.4166  0.024      90   0.395162 -1.928064                0.245788   \n",
       "\n",
       "   A0_sum_flux  A0_mean_flux  A0_std_detected  A1_mean_detected  \\\n",
       "0   -205.03693     -3.254554           0.3528            0.9653   \n",
       "1   -190.42786     -2.720398           0.3525            0.2678   \n",
       "2     -3.46179     -0.048080           0.0000            0.0000   \n",
       "3    129.42166      1.797523           0.1655            0.1250   \n",
       "4     41.63972      0.660948           0.0000            0.2241   \n",
       "\n",
       "   A2_sum_detected  A4_mean_detected  A5_std_detected  A5_mean_detected  \\\n",
       "0               57            0.9830           0.2854            0.9120   \n",
       "1               15            0.0893           0.0000            0.0000   \n",
       "2                7            0.0980           0.2715            0.0784   \n",
       "3               16            0.2322           0.3364            0.1273   \n",
       "4               18            0.1724           0.2578            0.0702   \n",
       "\n",
       "   percent_p2_region_minus_1  A2_min_flux  A5_sum_detected  \\\n",
       "0                      0.362     -682.000               52   \n",
       "1                      0.250      -10.070                0   \n",
       "2                      0.769       -2.850                4   \n",
       "3                      0.768       -2.160                7   \n",
       "4                      0.707       -2.084                4   \n",
       "\n",
       "   __flux_percentile_ratio_mid50___5_  __flux_percentile_ratio_mid65___2_  \\\n",
       "0                        5.562230e-26                        6.719410e-20   \n",
       "1                        2.119070e-02                        8.243180e-02   \n",
       "2                        1.942280e-04                        5.511800e-01   \n",
       "3                        8.401600e-03                        5.463690e-01   \n",
       "4                        2.702260e-02                        6.018110e-01   \n",
       "\n",
       "   __median_absolute_deviation___2_  __qso_log_chi2_qsonu___0_  \\\n",
       "0                         368.12900                   6.217890   \n",
       "1                           5.10035                   2.187190   \n",
       "2                           1.04253                  -0.307228   \n",
       "3                           1.41645                   1.432200   \n",
       "4                           1.33779                  -0.064359   \n",
       "\n",
       "   __stetson_k___1_  __freq1_signif___2_  __stetson_k___2_  \\\n",
       "0          1.091730              5.49891          1.053490   \n",
       "1          1.066100              3.95669          1.088180   \n",
       "2          0.933091              4.61663          0.634723   \n",
       "3          0.295163              3.96789          0.394683   \n",
       "4          0.674119              5.24444          0.560453   \n",
       "\n",
       "   __freq3_amplitude1___1_  __median_absolute_deviation___2_.1  \\\n",
       "0               114.465000                           368.12900   \n",
       "1                 0.851103                             5.10035   \n",
       "2                 0.454918                             1.04253   \n",
       "3                 3.595670                             1.41645   \n",
       "4                 1.205580                             1.33779   \n",
       "\n",
       "   __percent_close_to_median___2_  __freq_varrat___5_  __freq_varrat___4_  \\\n",
       "0                        0.172414            0.401664            0.129578   \n",
       "1                        0.178571            0.369518            0.166179   \n",
       "2                        0.769231            0.500549            0.318256   \n",
       "3                        0.892857            0.489589            0.360868   \n",
       "4                        0.741379            0.673592            0.382847   \n",
       "\n",
       "   __qso_log_chi2_qsonu___3_  __qso_log_chi2_qsonu___1_  \\\n",
       "0                    9.16612                   9.508750   \n",
       "1                    2.79753                   3.124810   \n",
       "2                    3.04833                   0.127758   \n",
       "3                    6.06886                   5.840820   \n",
       "4                    5.07231                   3.407900   \n",
       "\n",
       "   __qso_log_chi2_qsonu___5_  __std___4_  __freq_varrat___3_  \\\n",
       "0                   7.344980   289.27700            0.110785   \n",
       "1                   0.659762     6.34953            0.111883   \n",
       "2                   1.669430    10.60480            0.292954   \n",
       "3                   2.820440    32.77250            0.290652   \n",
       "4                   2.312920    26.63330            0.250639   \n",
       "\n",
       "   __amplitude___2_  outlierScore      hipd  lipd  \\\n",
       "0          646.9220         0.000  1.000000   1.0   \n",
       "1           10.2985         0.875  1.909016   2.0   \n",
       "2           11.9218         0.000  1.000000   1.0   \n",
       "3          111.4770         0.000  1.000000   1.0   \n",
       "4           54.3781         0.375  1.000000   1.0   \n",
       "\n",
       "   highEnergy_transitory_1.0_TF  highEnergy_transitory_1.5_TF  \\\n",
       "0                             0                             0   \n",
       "1                             1                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   lowEnergy_transitory_1.0_TF  lowEnergy_transitory_1.5_TF  A1_minus_3_sigma  \\\n",
       "0                            0                            0      -2191.161900   \n",
       "1                            1                            1        -18.157002   \n",
       "2                            0                            0         -5.280586   \n",
       "3                            0                            0        -72.173977   \n",
       "4                            1                            0        -19.689575   \n",
       "\n",
       "   A5_max_median_diff_flux  A5_minus_3_sigma  A5_max_mean_diff_flux  \\\n",
       "0               463.712450       -931.788350             425.637990   \n",
       "1                17.233897        -23.076394              16.565061   \n",
       "2                44.767413        -35.458878              42.770664   \n",
       "3               138.763735        -67.434252             130.767152   \n",
       "4               107.184313        -57.407143             102.260844   \n",
       "\n",
       "   diff_A5_A4_max_min_flux  diff_A2_A1_max_min_flux  \\\n",
       "0                -3.135000              -466.641780   \n",
       "1                 6.867315                -0.249980   \n",
       "2                19.473581                14.758601   \n",
       "3               -47.364906                27.090910   \n",
       "4               -40.125818                68.962513   \n",
       "\n",
       "   diff_A3_A2_median_min_flux  diff_A5_A4_max_median_flux  \\\n",
       "0                    -47.8000                  -21.782660   \n",
       "1                      2.3741                    6.613788   \n",
       "2                      2.7569                    4.611787   \n",
       "3                      3.7217                  -43.599234   \n",
       "4                      0.8410                  -34.670865   \n",
       "\n",
       "   diff_A4_A3_max_median_flux  diff_A2_A0_median_min_flux  \\\n",
       "0                 -122.341950                  309.302005   \n",
       "1                   -0.784106                   -1.686373   \n",
       "2                    7.244126                   -0.142353   \n",
       "3                  -19.526731                   -2.346763   \n",
       "4                    3.050778                   -4.412730   \n",
       "\n",
       "   diff_A4_A3_max_mean_flux  diff_A5_A2_max_mean_flux  \\\n",
       "0               -128.932226               -320.493130   \n",
       "1                 -1.589087                  5.241782   \n",
       "2                  6.515316                 24.176824   \n",
       "3                -18.339091                -80.316526   \n",
       "4                  4.962243                  5.833122   \n",
       "\n",
       "   diff_A5_A3_max_mean_flux  diff_A4_A0_median_mean_flux  \\\n",
       "0               -141.202570                   -40.826106   \n",
       "1                  4.247778                     0.484493   \n",
       "2                 12.434728                    -3.376547   \n",
       "3                -58.070623                   -11.123477   \n",
       "4                -26.471001                    -8.081169   \n",
       "\n",
       "   diff_A5_A4_max_mean_flux  diff_A2_A1_max_median_flux  \\\n",
       "0                -12.270344                 -270.841780   \n",
       "1                  5.836865                    0.956520   \n",
       "2                  5.919412                   14.981301   \n",
       "3                -39.731532                   29.014610   \n",
       "4                -31.433244                   69.766513   \n",
       "\n",
       "   diff_A5_A2_max_median_flux  diff_A5_A4_median_min_flux  \\\n",
       "0                 -414.072110                   18.647660   \n",
       "1                    6.586856                    0.253527   \n",
       "2                   24.264403                   14.861794   \n",
       "3                  -81.607175                   -3.765671   \n",
       "4                    1.401623                   -5.454953   \n",
       "\n",
       "   diff_A4_A0_median_min_flux  diff_A4_A1_max_median_flux  \\\n",
       "0                  211.745135                 -663.131230   \n",
       "1                   -0.143748                    0.929589   \n",
       "2                    3.356611                   34.633917   \n",
       "3                   11.833550                   -8.993331   \n",
       "4                   10.903035                  105.839001   \n",
       "\n",
       "   diff_A4_A2_max_median_flux  diff_A5_A4_minus_1_sigma  \\\n",
       "0                 -392.289450                  5.528684   \n",
       "1                   -0.026931                 -1.580997   \n",
       "2                   19.652616                 -2.391746   \n",
       "3                  -38.007941                  4.620626   \n",
       "4                   36.072488                  2.421924   \n",
       "\n",
       "   diff_A5_A3_median_min_flux  diff_A5_A3_max_median_flux  \\\n",
       "0                  -31.109210                 -144.124610   \n",
       "1                   -0.577947                    5.829681   \n",
       "2                   15.603858                   11.855913   \n",
       "3                    6.692942                  -63.125965   \n",
       "4                    9.019812                  -31.620087   \n",
       "\n",
       "   diff_A3_A1_minus_1_sigma  diff_A3_A0_median_min_flux  \\\n",
       "0                531.058750                  261.502005   \n",
       "1                 -0.705045                    0.687727   \n",
       "2                 -3.289694                    2.614547   \n",
       "3                 -0.310113                    1.374937   \n",
       "4                -11.709569                   -3.571730   \n",
       "\n",
       "   diff_A3_A0_plus_1_sigma  diff_A1_A0_median_min_flux  \\\n",
       "0               133.631375                  505.102005   \n",
       "1                 1.070336                   -0.479873   \n",
       "2                 9.647359                    0.080347   \n",
       "3                43.208654                   -0.423063   \n",
       "4                34.336172                   -3.608730   \n",
       "\n",
       "   diff_A4_A2_mean_min_flux  diff_A5_A1_plus_1_sigma  \\\n",
       "0               -181.623534                31.342350   \n",
       "1                  2.110778                 0.607564   \n",
       "2                  4.894168                15.923825   \n",
       "3                 16.757367                 5.121609   \n",
       "4                 14.121887                15.590844   \n",
       "\n",
       "   diff_A4_A1_median_mean_flux  diff_A3_A2_max_median_flux  \\\n",
       "0                    54.613224                 -269.947500   \n",
       "1                    -0.350414                    0.757175   \n",
       "2                    -3.334674                   12.408490   \n",
       "3                    -7.032285                  -18.481210   \n",
       "4                    -4.682090                   33.021710   \n",
       "\n",
       "   diff_A5_A1_median_mean_flux  div_A4_A2_median_min_flux  \\\n",
       "0                    64.125540                   0.765601   \n",
       "1                    -1.127337                   1.155007   \n",
       "2                    -2.027049                   2.047061   \n",
       "3                    -3.164583                   6.487100   \n",
       "4                    -1.444469                   6.151620   \n",
       "\n",
       "   div_A5_A2_median_min_flux  div_A5_A2_minus_1_sigma  \\\n",
       "0                   0.810406                 0.580770   \n",
       "1                   1.180482                 1.353889   \n",
       "2                   6.494436                 2.783931   \n",
       "3                   5.029966                 0.688379   \n",
       "4                   4.316788                 1.312571   \n",
       "\n",
       "   div_A5_A4_median_mean_flux  div_A3_A0_plus_1_sigma  div_A4_A1_minus_1_sigma  \n",
       "0                    0.800106                2.656105                 0.352129  \n",
       "1                   -6.188008                1.243640                 1.085396  \n",
       "2                    0.604275                6.417455                 3.842210  \n",
       "3                    0.674005                8.000790                 0.984685  \n",
       "4                    0.603286               12.365708                 4.881188  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metadata_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>flux_min</th>\n",
       "      <th>flux_max</th>\n",
       "      <th>flux_mean</th>\n",
       "      <th>flux_median</th>\n",
       "      <th>flux_std</th>\n",
       "      <th>flux_skew</th>\n",
       "      <th>flux_err_min</th>\n",
       "      <th>flux_err_max</th>\n",
       "      <th>flux_err_mean</th>\n",
       "      <th>flux_err_median</th>\n",
       "      <th>flux_err_std</th>\n",
       "      <th>flux_err_skew</th>\n",
       "      <th>detected_mean</th>\n",
       "      <th>flux_ratio_sq_sum</th>\n",
       "      <th>flux_ratio_sq_skew</th>\n",
       "      <th>flux_by_flux_ratio_sq_sum</th>\n",
       "      <th>flux_by_flux_ratio_sq_skew</th>\n",
       "      <th>flux_w_mean</th>\n",
       "      <th>flux_diff1</th>\n",
       "      <th>flux_diff2</th>\n",
       "      <th>flux_diff3</th>\n",
       "      <th>0__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>0__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>0__kurtosis</th>\n",
       "      <th>0__skewness</th>\n",
       "      <th>1__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>1__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>1__kurtosis</th>\n",
       "      <th>1__skewness</th>\n",
       "      <th>2__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>2__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>2__kurtosis</th>\n",
       "      <th>2__skewness</th>\n",
       "      <th>3__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>3__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>3__kurtosis</th>\n",
       "      <th>3__skewness</th>\n",
       "      <th>4__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>4__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>4__kurtosis</th>\n",
       "      <th>4__skewness</th>\n",
       "      <th>5__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>5__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>5__kurtosis</th>\n",
       "      <th>5__skewness</th>\n",
       "      <th>flux__length</th>\n",
       "      <th>flux__longest_strike_above_mean</th>\n",
       "      <th>flux__longest_strike_below_mean</th>\n",
       "      <th>flux__mean_abs_change</th>\n",
       "      <th>flux__mean_change</th>\n",
       "      <th>flux_by_flux_ratio_sq__longest_strike_above_mean</th>\n",
       "      <th>flux_by_flux_ratio_sq__longest_strike_below_mean</th>\n",
       "      <th>mjd__mean_abs_change</th>\n",
       "      <th>mjd__mean_change</th>\n",
       "      <th>mjd_diff_det</th>\n",
       "      <th>hostgal_photoz</th>\n",
       "      <th>hostgal_photoz_err</th>\n",
       "      <th>distmod</th>\n",
       "      <th>mwebv</th>\n",
       "      <th>haversine</th>\n",
       "      <th>latlon1</th>\n",
       "      <th>hostgal_photoz_certain</th>\n",
       "      <th>A0_sum_flux</th>\n",
       "      <th>A0_mean_flux</th>\n",
       "      <th>A0_std_detected</th>\n",
       "      <th>A1_mean_detected</th>\n",
       "      <th>A2_sum_detected</th>\n",
       "      <th>A4_mean_detected</th>\n",
       "      <th>A5_std_detected</th>\n",
       "      <th>A5_mean_detected</th>\n",
       "      <th>percent_p2_region_minus_1</th>\n",
       "      <th>A2_min_flux</th>\n",
       "      <th>A5_sum_detected</th>\n",
       "      <th>__flux_percentile_ratio_mid50___5_</th>\n",
       "      <th>__flux_percentile_ratio_mid65___2_</th>\n",
       "      <th>__median_absolute_deviation___2_</th>\n",
       "      <th>__qso_log_chi2_qsonu___0_</th>\n",
       "      <th>__stetson_k___1_</th>\n",
       "      <th>__freq1_signif___2_</th>\n",
       "      <th>__stetson_k___2_</th>\n",
       "      <th>__freq3_amplitude1___1_</th>\n",
       "      <th>__median_absolute_deviation___2_.1</th>\n",
       "      <th>__percent_close_to_median___2_</th>\n",
       "      <th>__freq_varrat___5_</th>\n",
       "      <th>__freq_varrat___4_</th>\n",
       "      <th>__qso_log_chi2_qsonu___3_</th>\n",
       "      <th>__qso_log_chi2_qsonu___1_</th>\n",
       "      <th>__qso_log_chi2_qsonu___5_</th>\n",
       "      <th>__std___4_</th>\n",
       "      <th>__freq_varrat___3_</th>\n",
       "      <th>__amplitude___2_</th>\n",
       "      <th>outlierScore</th>\n",
       "      <th>hipd</th>\n",
       "      <th>lipd</th>\n",
       "      <th>highEnergy_transitory_1.0_TF</th>\n",
       "      <th>highEnergy_transitory_1.5_TF</th>\n",
       "      <th>lowEnergy_transitory_1.0_TF</th>\n",
       "      <th>lowEnergy_transitory_1.5_TF</th>\n",
       "      <th>A1_minus_3_sigma</th>\n",
       "      <th>A5_max_median_diff_flux</th>\n",
       "      <th>A5_minus_3_sigma</th>\n",
       "      <th>A5_max_mean_diff_flux</th>\n",
       "      <th>diff_A5_A4_max_min_flux</th>\n",
       "      <th>diff_A2_A1_max_min_flux</th>\n",
       "      <th>diff_A3_A2_median_min_flux</th>\n",
       "      <th>diff_A5_A4_max_median_flux</th>\n",
       "      <th>diff_A4_A3_max_median_flux</th>\n",
       "      <th>diff_A2_A0_median_min_flux</th>\n",
       "      <th>diff_A4_A3_max_mean_flux</th>\n",
       "      <th>diff_A5_A2_max_mean_flux</th>\n",
       "      <th>diff_A5_A3_max_mean_flux</th>\n",
       "      <th>diff_A4_A0_median_mean_flux</th>\n",
       "      <th>diff_A5_A4_max_mean_flux</th>\n",
       "      <th>diff_A2_A1_max_median_flux</th>\n",
       "      <th>diff_A5_A2_max_median_flux</th>\n",
       "      <th>diff_A5_A4_median_min_flux</th>\n",
       "      <th>diff_A4_A0_median_min_flux</th>\n",
       "      <th>diff_A4_A1_max_median_flux</th>\n",
       "      <th>diff_A4_A2_max_median_flux</th>\n",
       "      <th>diff_A5_A4_minus_1_sigma</th>\n",
       "      <th>diff_A5_A3_median_min_flux</th>\n",
       "      <th>diff_A5_A3_max_median_flux</th>\n",
       "      <th>diff_A3_A1_minus_1_sigma</th>\n",
       "      <th>diff_A3_A0_median_min_flux</th>\n",
       "      <th>diff_A3_A0_plus_1_sigma</th>\n",
       "      <th>diff_A1_A0_median_min_flux</th>\n",
       "      <th>diff_A4_A2_mean_min_flux</th>\n",
       "      <th>diff_A5_A1_plus_1_sigma</th>\n",
       "      <th>diff_A4_A1_median_mean_flux</th>\n",
       "      <th>diff_A3_A2_max_median_flux</th>\n",
       "      <th>diff_A5_A1_median_mean_flux</th>\n",
       "      <th>div_A4_A2_median_min_flux</th>\n",
       "      <th>div_A5_A2_median_min_flux</th>\n",
       "      <th>div_A5_A2_minus_1_sigma</th>\n",
       "      <th>div_A5_A4_median_mean_flux</th>\n",
       "      <th>div_A3_A0_plus_1_sigma</th>\n",
       "      <th>div_A4_A1_minus_1_sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>-12.680235</td>\n",
       "      <td>42.765503</td>\n",
       "      <td>3.997127</td>\n",
       "      <td>0.616561</td>\n",
       "      <td>9.149645</td>\n",
       "      <td>2.037355</td>\n",
       "      <td>0.691634</td>\n",
       "      <td>11.257108</td>\n",
       "      <td>2.461810</td>\n",
       "      <td>1.972972</td>\n",
       "      <td>1.718101</td>\n",
       "      <td>1.826388</td>\n",
       "      <td>0.157576</td>\n",
       "      <td>7806.412424</td>\n",
       "      <td>4.771625</td>\n",
       "      <td>1.896346e+05</td>\n",
       "      <td>5.396523</td>\n",
       "      <td>24.292155</td>\n",
       "      <td>55.445738</td>\n",
       "      <td>13.871398</td>\n",
       "      <td>2.282455</td>\n",
       "      <td>29.002872</td>\n",
       "      <td>37.684425</td>\n",
       "      <td>-0.247160</td>\n",
       "      <td>0.147622</td>\n",
       "      <td>92.039690</td>\n",
       "      <td>79.990817</td>\n",
       "      <td>10.567412</td>\n",
       "      <td>3.117684</td>\n",
       "      <td>164.640622</td>\n",
       "      <td>139.733762</td>\n",
       "      <td>4.512783</td>\n",
       "      <td>2.171455</td>\n",
       "      <td>300.547278</td>\n",
       "      <td>246.788411</td>\n",
       "      <td>2.951479</td>\n",
       "      <td>1.806797</td>\n",
       "      <td>408.305525</td>\n",
       "      <td>349.008202</td>\n",
       "      <td>0.094963</td>\n",
       "      <td>1.153076</td>\n",
       "      <td>324.515880</td>\n",
       "      <td>251.116361</td>\n",
       "      <td>0.072799</td>\n",
       "      <td>0.804929</td>\n",
       "      <td>330.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.426774</td>\n",
       "      <td>0.017885</td>\n",
       "      <td>9.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>2.350061</td>\n",
       "      <td>2.350061</td>\n",
       "      <td>119.8531</td>\n",
       "      <td>0.3193</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>41.1123</td>\n",
       "      <td>0.019</td>\n",
       "      <td>1.851382</td>\n",
       "      <td>3.049709</td>\n",
       "      <td>0.337084</td>\n",
       "      <td>29.002872</td>\n",
       "      <td>0.402818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.07690</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.3137</td>\n",
       "      <td>0.3254</td>\n",
       "      <td>0.11770</td>\n",
       "      <td>0.7114</td>\n",
       "      <td>-1.778855</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.005627</td>\n",
       "      <td>0.528087</td>\n",
       "      <td>1.384690</td>\n",
       "      <td>-0.007597</td>\n",
       "      <td>0.610190</td>\n",
       "      <td>4.81984</td>\n",
       "      <td>0.681284</td>\n",
       "      <td>1.783390</td>\n",
       "      <td>1.384690</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.255030</td>\n",
       "      <td>0.090550</td>\n",
       "      <td>3.499440</td>\n",
       "      <td>2.788160</td>\n",
       "      <td>1.510790</td>\n",
       "      <td>13.24940</td>\n",
       "      <td>0.124063</td>\n",
       "      <td>13.15420</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-13.032169</td>\n",
       "      <td>33.023636</td>\n",
       "      <td>-29.949324</td>\n",
       "      <td>30.439033</td>\n",
       "      <td>1.203725</td>\n",
       "      <td>-1.244573</td>\n",
       "      <td>3.146350</td>\n",
       "      <td>-9.275401</td>\n",
       "      <td>3.413229</td>\n",
       "      <td>-1.906972</td>\n",
       "      <td>0.572977</td>\n",
       "      <td>9.075555</td>\n",
       "      <td>-3.747502</td>\n",
       "      <td>-7.224033</td>\n",
       "      <td>-4.320479</td>\n",
       "      <td>1.245629</td>\n",
       "      <td>8.963043</td>\n",
       "      <td>10.479126</td>\n",
       "      <td>1.824685</td>\n",
       "      <td>19.484073</td>\n",
       "      <td>18.238444</td>\n",
       "      <td>-0.365809</td>\n",
       "      <td>11.064433</td>\n",
       "      <td>-5.862173</td>\n",
       "      <td>-1.251189</td>\n",
       "      <td>1.239378</td>\n",
       "      <td>13.511033</td>\n",
       "      <td>0.583230</td>\n",
       "      <td>8.574067</td>\n",
       "      <td>11.763127</td>\n",
       "      <td>-6.359525</td>\n",
       "      <td>14.825216</td>\n",
       "      <td>-1.404603</td>\n",
       "      <td>2.660059</td>\n",
       "      <td>7.321785</td>\n",
       "      <td>1.927288</td>\n",
       "      <td>0.342807</td>\n",
       "      <td>6.483993</td>\n",
       "      <td>1.698851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>-11.142164</td>\n",
       "      <td>14.839427</td>\n",
       "      <td>0.884047</td>\n",
       "      <td>0.072856</td>\n",
       "      <td>3.399946</td>\n",
       "      <td>0.970525</td>\n",
       "      <td>0.690589</td>\n",
       "      <td>11.249375</td>\n",
       "      <td>2.457580</td>\n",
       "      <td>1.973559</td>\n",
       "      <td>1.717591</td>\n",
       "      <td>1.826703</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>806.406927</td>\n",
       "      <td>11.486148</td>\n",
       "      <td>5.525817e+03</td>\n",
       "      <td>12.348124</td>\n",
       "      <td>6.852393</td>\n",
       "      <td>25.981591</td>\n",
       "      <td>29.389389</td>\n",
       "      <td>3.791608</td>\n",
       "      <td>22.708482</td>\n",
       "      <td>26.159787</td>\n",
       "      <td>0.099267</td>\n",
       "      <td>0.502325</td>\n",
       "      <td>13.685195</td>\n",
       "      <td>27.630359</td>\n",
       "      <td>13.429229</td>\n",
       "      <td>3.109318</td>\n",
       "      <td>31.012899</td>\n",
       "      <td>33.427074</td>\n",
       "      <td>13.769006</td>\n",
       "      <td>3.247873</td>\n",
       "      <td>56.042403</td>\n",
       "      <td>59.784625</td>\n",
       "      <td>1.750840</td>\n",
       "      <td>1.082798</td>\n",
       "      <td>83.561278</td>\n",
       "      <td>77.494564</td>\n",
       "      <td>-0.322108</td>\n",
       "      <td>-0.099957</td>\n",
       "      <td>84.725142</td>\n",
       "      <td>27.013154</td>\n",
       "      <td>-0.348744</td>\n",
       "      <td>0.132025</td>\n",
       "      <td>330.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.055953</td>\n",
       "      <td>-0.025935</td>\n",
       "      <td>4.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>9.335100</td>\n",
       "      <td>9.335100</td>\n",
       "      <td>28.0053</td>\n",
       "      <td>0.6323</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>42.8774</td>\n",
       "      <td>0.018</td>\n",
       "      <td>1.855173</td>\n",
       "      <td>3.009107</td>\n",
       "      <td>0.643720</td>\n",
       "      <td>22.708480</td>\n",
       "      <td>0.315396</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.01923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.6150</td>\n",
       "      <td>-2.655194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014907</td>\n",
       "      <td>0.278418</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.266805</td>\n",
       "      <td>0.640702</td>\n",
       "      <td>3.31422</td>\n",
       "      <td>0.665239</td>\n",
       "      <td>1.072370</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.623417</td>\n",
       "      <td>0.258451</td>\n",
       "      <td>0.848093</td>\n",
       "      <td>1.339580</td>\n",
       "      <td>-0.158567</td>\n",
       "      <td>3.44694</td>\n",
       "      <td>0.306123</td>\n",
       "      <td>8.56024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.039201</td>\n",
       "      <td>13.705170</td>\n",
       "      <td>-14.805055</td>\n",
       "      <td>13.178150</td>\n",
       "      <td>10.362185</td>\n",
       "      <td>-1.201913</td>\n",
       "      <td>1.962808</td>\n",
       "      <td>6.524891</td>\n",
       "      <td>-2.977615</td>\n",
       "      <td>-1.396471</td>\n",
       "      <td>-1.799531</td>\n",
       "      <td>-0.690726</td>\n",
       "      <td>3.786349</td>\n",
       "      <td>0.761845</td>\n",
       "      <td>5.585880</td>\n",
       "      <td>0.557757</td>\n",
       "      <td>-0.730177</td>\n",
       "      <td>3.837294</td>\n",
       "      <td>4.357531</td>\n",
       "      <td>-6.697311</td>\n",
       "      <td>-7.255068</td>\n",
       "      <td>-1.984715</td>\n",
       "      <td>7.628487</td>\n",
       "      <td>3.547276</td>\n",
       "      <td>0.777005</td>\n",
       "      <td>0.566338</td>\n",
       "      <td>1.195116</td>\n",
       "      <td>0.363199</td>\n",
       "      <td>4.775541</td>\n",
       "      <td>4.119388</td>\n",
       "      <td>0.804491</td>\n",
       "      <td>-4.277454</td>\n",
       "      <td>-0.134520</td>\n",
       "      <td>3.142918</td>\n",
       "      <td>4.572011</td>\n",
       "      <td>1.764152</td>\n",
       "      <td>-1.279205</td>\n",
       "      <td>1.444674</td>\n",
       "      <td>0.735858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>-14.202744</td>\n",
       "      <td>16.761280</td>\n",
       "      <td>0.791032</td>\n",
       "      <td>0.458390</td>\n",
       "      <td>3.886578</td>\n",
       "      <td>0.377131</td>\n",
       "      <td>0.663680</td>\n",
       "      <td>11.278636</td>\n",
       "      <td>2.702947</td>\n",
       "      <td>2.184483</td>\n",
       "      <td>1.922641</td>\n",
       "      <td>1.802497</td>\n",
       "      <td>0.014205</td>\n",
       "      <td>784.835502</td>\n",
       "      <td>9.509911</td>\n",
       "      <td>4.124400e+03</td>\n",
       "      <td>9.923556</td>\n",
       "      <td>5.255113</td>\n",
       "      <td>30.964024</td>\n",
       "      <td>39.143819</td>\n",
       "      <td>5.892170</td>\n",
       "      <td>6.030862</td>\n",
       "      <td>46.450439</td>\n",
       "      <td>0.590786</td>\n",
       "      <td>-0.427429</td>\n",
       "      <td>53.366119</td>\n",
       "      <td>41.192678</td>\n",
       "      <td>4.127763</td>\n",
       "      <td>1.710048</td>\n",
       "      <td>58.607170</td>\n",
       "      <td>42.353288</td>\n",
       "      <td>8.642889</td>\n",
       "      <td>2.570936</td>\n",
       "      <td>26.975615</td>\n",
       "      <td>59.958041</td>\n",
       "      <td>3.484929</td>\n",
       "      <td>1.141959</td>\n",
       "      <td>57.222812</td>\n",
       "      <td>72.448018</td>\n",
       "      <td>2.085327</td>\n",
       "      <td>0.429582</td>\n",
       "      <td>76.240782</td>\n",
       "      <td>73.533783</td>\n",
       "      <td>0.063837</td>\n",
       "      <td>-0.175525</td>\n",
       "      <td>352.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.247664</td>\n",
       "      <td>0.024840</td>\n",
       "      <td>4.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>2.7628</td>\n",
       "      <td>0.8297</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>43.6000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.309914</td>\n",
       "      <td>-1.490290</td>\n",
       "      <td>0.881446</td>\n",
       "      <td>6.030863</td>\n",
       "      <td>0.095728</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.03450</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.6206</td>\n",
       "      <td>-2.790141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.323662</td>\n",
       "      <td>0.845784</td>\n",
       "      <td>0.335972</td>\n",
       "      <td>0.798601</td>\n",
       "      <td>3.18365</td>\n",
       "      <td>0.696152</td>\n",
       "      <td>1.038100</td>\n",
       "      <td>0.845784</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.669229</td>\n",
       "      <td>0.427519</td>\n",
       "      <td>0.684981</td>\n",
       "      <td>1.213800</td>\n",
       "      <td>0.066402</td>\n",
       "      <td>3.80032</td>\n",
       "      <td>0.493005</td>\n",
       "      <td>7.88173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.832717</td>\n",
       "      <td>14.683493</td>\n",
       "      <td>-19.129198</td>\n",
       "      <td>15.423722</td>\n",
       "      <td>8.578852</td>\n",
       "      <td>0.193475</td>\n",
       "      <td>2.646153</td>\n",
       "      <td>3.214523</td>\n",
       "      <td>1.212961</td>\n",
       "      <td>-4.544296</td>\n",
       "      <td>0.695796</td>\n",
       "      <td>3.460874</td>\n",
       "      <td>4.996827</td>\n",
       "      <td>-0.503209</td>\n",
       "      <td>4.301032</td>\n",
       "      <td>1.040025</td>\n",
       "      <td>2.092920</td>\n",
       "      <td>5.364329</td>\n",
       "      <td>3.199021</td>\n",
       "      <td>-0.081578</td>\n",
       "      <td>-1.121603</td>\n",
       "      <td>-2.637786</td>\n",
       "      <td>10.461493</td>\n",
       "      <td>4.427483</td>\n",
       "      <td>-0.369331</td>\n",
       "      <td>-1.898143</td>\n",
       "      <td>0.430738</td>\n",
       "      <td>-3.697746</td>\n",
       "      <td>7.461872</td>\n",
       "      <td>4.322237</td>\n",
       "      <td>0.368021</td>\n",
       "      <td>-2.334563</td>\n",
       "      <td>1.454529</td>\n",
       "      <td>3.440466</td>\n",
       "      <td>5.131144</td>\n",
       "      <td>3.490031</td>\n",
       "      <td>-2.137666</td>\n",
       "      <td>1.150274</td>\n",
       "      <td>1.425184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>-12.631923</td>\n",
       "      <td>28.061138</td>\n",
       "      <td>0.970396</td>\n",
       "      <td>0.465986</td>\n",
       "      <td>4.100713</td>\n",
       "      <td>2.177402</td>\n",
       "      <td>0.697639</td>\n",
       "      <td>11.305429</td>\n",
       "      <td>2.480364</td>\n",
       "      <td>2.003704</td>\n",
       "      <td>1.723367</td>\n",
       "      <td>1.821626</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>876.027511</td>\n",
       "      <td>8.010597</td>\n",
       "      <td>8.293673e+03</td>\n",
       "      <td>9.227223</td>\n",
       "      <td>9.467365</td>\n",
       "      <td>40.693061</td>\n",
       "      <td>41.934474</td>\n",
       "      <td>4.298245</td>\n",
       "      <td>0.776942</td>\n",
       "      <td>14.926490</td>\n",
       "      <td>-0.580529</td>\n",
       "      <td>-0.058688</td>\n",
       "      <td>2.371129</td>\n",
       "      <td>22.476077</td>\n",
       "      <td>1.429683</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>34.447730</td>\n",
       "      <td>14.240274</td>\n",
       "      <td>1.223998</td>\n",
       "      <td>0.468594</td>\n",
       "      <td>58.911457</td>\n",
       "      <td>62.709899</td>\n",
       "      <td>4.269139</td>\n",
       "      <td>1.414380</td>\n",
       "      <td>115.918277</td>\n",
       "      <td>70.371095</td>\n",
       "      <td>7.377841</td>\n",
       "      <td>2.409871</td>\n",
       "      <td>112.547521</td>\n",
       "      <td>123.413344</td>\n",
       "      <td>1.793238</td>\n",
       "      <td>0.953107</td>\n",
       "      <td>330.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.299337</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>4.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>3.0125</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>42.9640</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1.845038</td>\n",
       "      <td>3.078301</td>\n",
       "      <td>0.757434</td>\n",
       "      <td>0.776942</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.3845</td>\n",
       "      <td>-4.254661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.402374</td>\n",
       "      <td>0.906290</td>\n",
       "      <td>0.039595</td>\n",
       "      <td>0.918965</td>\n",
       "      <td>3.55265</td>\n",
       "      <td>0.934093</td>\n",
       "      <td>0.445288</td>\n",
       "      <td>0.906290</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.709745</td>\n",
       "      <td>0.405740</td>\n",
       "      <td>1.512120</td>\n",
       "      <td>0.154437</td>\n",
       "      <td>0.664241</td>\n",
       "      <td>4.61269</td>\n",
       "      <td>0.415919</td>\n",
       "      <td>4.82121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.651117</td>\n",
       "      <td>26.624370</td>\n",
       "      <td>-20.850721</td>\n",
       "      <td>25.854324</td>\n",
       "      <td>15.809828</td>\n",
       "      <td>-0.666392</td>\n",
       "      <td>1.807202</td>\n",
       "      <td>6.525865</td>\n",
       "      <td>8.105404</td>\n",
       "      <td>0.318818</td>\n",
       "      <td>7.272845</td>\n",
       "      <td>21.129028</td>\n",
       "      <td>14.127387</td>\n",
       "      <td>-0.958548</td>\n",
       "      <td>6.854541</td>\n",
       "      <td>-0.398379</td>\n",
       "      <td>21.839384</td>\n",
       "      <td>9.283963</td>\n",
       "      <td>0.246119</td>\n",
       "      <td>14.915140</td>\n",
       "      <td>15.313519</td>\n",
       "      <td>-3.093347</td>\n",
       "      <td>7.404063</td>\n",
       "      <td>14.631269</td>\n",
       "      <td>-0.262982</td>\n",
       "      <td>2.126019</td>\n",
       "      <td>2.369858</td>\n",
       "      <td>0.586831</td>\n",
       "      <td>0.966333</td>\n",
       "      <td>8.069753</td>\n",
       "      <td>-1.239923</td>\n",
       "      <td>7.208114</td>\n",
       "      <td>-0.911246</td>\n",
       "      <td>0.985034</td>\n",
       "      <td>2.896326</td>\n",
       "      <td>4.842085</td>\n",
       "      <td>0.700856</td>\n",
       "      <td>2.143172</td>\n",
       "      <td>1.246370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>-13.239577</td>\n",
       "      <td>124.475609</td>\n",
       "      <td>4.580070</td>\n",
       "      <td>0.301366</td>\n",
       "      <td>19.862714</td>\n",
       "      <td>4.537527</td>\n",
       "      <td>0.679312</td>\n",
       "      <td>11.365292</td>\n",
       "      <td>2.746784</td>\n",
       "      <td>2.210577</td>\n",
       "      <td>1.930790</td>\n",
       "      <td>1.798629</td>\n",
       "      <td>0.065341</td>\n",
       "      <td>47612.580669</td>\n",
       "      <td>9.140568</td>\n",
       "      <td>4.815012e+06</td>\n",
       "      <td>10.251332</td>\n",
       "      <td>101.128982</td>\n",
       "      <td>137.715186</td>\n",
       "      <td>30.068359</td>\n",
       "      <td>1.361778</td>\n",
       "      <td>65.064185</td>\n",
       "      <td>55.191226</td>\n",
       "      <td>7.457425</td>\n",
       "      <td>2.135457</td>\n",
       "      <td>202.641918</td>\n",
       "      <td>191.410916</td>\n",
       "      <td>16.393710</td>\n",
       "      <td>4.133929</td>\n",
       "      <td>425.940451</td>\n",
       "      <td>414.455381</td>\n",
       "      <td>15.206929</td>\n",
       "      <td>4.036551</td>\n",
       "      <td>368.228927</td>\n",
       "      <td>360.630612</td>\n",
       "      <td>15.386561</td>\n",
       "      <td>4.030676</td>\n",
       "      <td>377.744797</td>\n",
       "      <td>301.983950</td>\n",
       "      <td>15.302549</td>\n",
       "      <td>3.992740</td>\n",
       "      <td>172.564336</td>\n",
       "      <td>289.757599</td>\n",
       "      <td>12.507289</td>\n",
       "      <td>3.513977</td>\n",
       "      <td>352.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.054175</td>\n",
       "      <td>-0.026764</td>\n",
       "      <td>10.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1.138964</td>\n",
       "      <td>1.138964</td>\n",
       "      <td>25.0572</td>\n",
       "      <td>0.4617</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>42.0540</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.391772</td>\n",
       "      <td>-1.993709</td>\n",
       "      <td>0.467367</td>\n",
       "      <td>65.064186</td>\n",
       "      <td>1.032765</td>\n",
       "      <td>0.1768</td>\n",
       "      <td>0.06900</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.2253</td>\n",
       "      <td>0.05264</td>\n",
       "      <td>0.9136</td>\n",
       "      <td>-3.601765</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.597406</td>\n",
       "      <td>1.026020</td>\n",
       "      <td>0.976849</td>\n",
       "      <td>0.393851</td>\n",
       "      <td>3.96443</td>\n",
       "      <td>0.359669</td>\n",
       "      <td>4.311070</td>\n",
       "      <td>1.026020</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.813533</td>\n",
       "      <td>0.614606</td>\n",
       "      <td>5.150450</td>\n",
       "      <td>4.771000</td>\n",
       "      <td>2.482000</td>\n",
       "      <td>22.24760</td>\n",
       "      <td>0.534743</td>\n",
       "      <td>64.03870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-38.006542</td>\n",
       "      <td>91.693475</td>\n",
       "      <td>-58.217627</td>\n",
       "      <td>87.733245</td>\n",
       "      <td>-11.865965</td>\n",
       "      <td>53.806883</td>\n",
       "      <td>0.909528</td>\n",
       "      <td>-18.816458</td>\n",
       "      <td>-1.051308</td>\n",
       "      <td>-3.743664</td>\n",
       "      <td>-0.873642</td>\n",
       "      <td>-29.398564</td>\n",
       "      <td>-18.151450</td>\n",
       "      <td>-5.048370</td>\n",
       "      <td>-17.277809</td>\n",
       "      <td>54.582170</td>\n",
       "      <td>-32.303375</td>\n",
       "      <td>6.950493</td>\n",
       "      <td>-2.467890</td>\n",
       "      <td>41.095253</td>\n",
       "      <td>-13.486917</td>\n",
       "      <td>-1.458527</td>\n",
       "      <td>7.316738</td>\n",
       "      <td>-19.867766</td>\n",
       "      <td>-6.951712</td>\n",
       "      <td>-2.834136</td>\n",
       "      <td>24.852925</td>\n",
       "      <td>-2.968377</td>\n",
       "      <td>-0.090388</td>\n",
       "      <td>6.114955</td>\n",
       "      <td>-2.169479</td>\n",
       "      <td>-12.435609</td>\n",
       "      <td>-0.630830</td>\n",
       "      <td>1.312649</td>\n",
       "      <td>3.015982</td>\n",
       "      <td>0.895981</td>\n",
       "      <td>0.720189</td>\n",
       "      <td>5.839107</td>\n",
       "      <td>1.540600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id   flux_min    flux_max  flux_mean  flux_median   flux_std  \\\n",
       "0         13 -12.680235   42.765503   3.997127     0.616561   9.149645   \n",
       "1         14 -11.142164   14.839427   0.884047     0.072856   3.399946   \n",
       "2         17 -14.202744   16.761280   0.791032     0.458390   3.886578   \n",
       "3         23 -12.631923   28.061138   0.970396     0.465986   4.100713   \n",
       "4         34 -13.239577  124.475609   4.580070     0.301366  19.862714   \n",
       "\n",
       "   flux_skew  flux_err_min  flux_err_max  flux_err_mean  flux_err_median  \\\n",
       "0   2.037355      0.691634     11.257108       2.461810         1.972972   \n",
       "1   0.970525      0.690589     11.249375       2.457580         1.973559   \n",
       "2   0.377131      0.663680     11.278636       2.702947         2.184483   \n",
       "3   2.177402      0.697639     11.305429       2.480364         2.003704   \n",
       "4   4.537527      0.679312     11.365292       2.746784         2.210577   \n",
       "\n",
       "   flux_err_std  flux_err_skew  detected_mean  flux_ratio_sq_sum  \\\n",
       "0      1.718101       1.826388       0.157576        7806.412424   \n",
       "1      1.717591       1.826703       0.012121         806.406927   \n",
       "2      1.922641       1.802497       0.014205         784.835502   \n",
       "3      1.723367       1.821626       0.018182         876.027511   \n",
       "4      1.930790       1.798629       0.065341       47612.580669   \n",
       "\n",
       "   flux_ratio_sq_skew  flux_by_flux_ratio_sq_sum  flux_by_flux_ratio_sq_skew  \\\n",
       "0            4.771625               1.896346e+05                    5.396523   \n",
       "1           11.486148               5.525817e+03                   12.348124   \n",
       "2            9.509911               4.124400e+03                    9.923556   \n",
       "3            8.010597               8.293673e+03                    9.227223   \n",
       "4            9.140568               4.815012e+06                   10.251332   \n",
       "\n",
       "   flux_w_mean  flux_diff1  flux_diff2  flux_diff3  \\\n",
       "0    24.292155   55.445738   13.871398    2.282455   \n",
       "1     6.852393   25.981591   29.389389    3.791608   \n",
       "2     5.255113   30.964024   39.143819    5.892170   \n",
       "3     9.467365   40.693061   41.934474    4.298245   \n",
       "4   101.128982  137.715186   30.068359    1.361778   \n",
       "\n",
       "   0__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                                29.002872   \n",
       "1                                22.708482   \n",
       "2                                 6.030862   \n",
       "3                                 0.776942   \n",
       "4                                65.064185   \n",
       "\n",
       "   0__fft_coefficient__coeff_1__attr_\"abs\"  0__kurtosis  0__skewness  \\\n",
       "0                                37.684425    -0.247160     0.147622   \n",
       "1                                26.159787     0.099267     0.502325   \n",
       "2                                46.450439     0.590786    -0.427429   \n",
       "3                                14.926490    -0.580529    -0.058688   \n",
       "4                                55.191226     7.457425     2.135457   \n",
       "\n",
       "   1__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                                92.039690   \n",
       "1                                13.685195   \n",
       "2                                53.366119   \n",
       "3                                 2.371129   \n",
       "4                               202.641918   \n",
       "\n",
       "   1__fft_coefficient__coeff_1__attr_\"abs\"  1__kurtosis  1__skewness  \\\n",
       "0                                79.990817    10.567412     3.117684   \n",
       "1                                27.630359    13.429229     3.109318   \n",
       "2                                41.192678     4.127763     1.710048   \n",
       "3                                22.476077     1.429683    -0.000571   \n",
       "4                               191.410916    16.393710     4.133929   \n",
       "\n",
       "   2__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                               164.640622   \n",
       "1                                31.012899   \n",
       "2                                58.607170   \n",
       "3                                34.447730   \n",
       "4                               425.940451   \n",
       "\n",
       "   2__fft_coefficient__coeff_1__attr_\"abs\"  2__kurtosis  2__skewness  \\\n",
       "0                               139.733762     4.512783     2.171455   \n",
       "1                                33.427074    13.769006     3.247873   \n",
       "2                                42.353288     8.642889     2.570936   \n",
       "3                                14.240274     1.223998     0.468594   \n",
       "4                               414.455381    15.206929     4.036551   \n",
       "\n",
       "   3__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                               300.547278   \n",
       "1                                56.042403   \n",
       "2                                26.975615   \n",
       "3                                58.911457   \n",
       "4                               368.228927   \n",
       "\n",
       "   3__fft_coefficient__coeff_1__attr_\"abs\"  3__kurtosis  3__skewness  \\\n",
       "0                               246.788411     2.951479     1.806797   \n",
       "1                                59.784625     1.750840     1.082798   \n",
       "2                                59.958041     3.484929     1.141959   \n",
       "3                                62.709899     4.269139     1.414380   \n",
       "4                               360.630612    15.386561     4.030676   \n",
       "\n",
       "   4__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                               408.305525   \n",
       "1                                83.561278   \n",
       "2                                57.222812   \n",
       "3                               115.918277   \n",
       "4                               377.744797   \n",
       "\n",
       "   4__fft_coefficient__coeff_1__attr_\"abs\"  4__kurtosis  4__skewness  \\\n",
       "0                               349.008202     0.094963     1.153076   \n",
       "1                                77.494564    -0.322108    -0.099957   \n",
       "2                                72.448018     2.085327     0.429582   \n",
       "3                                70.371095     7.377841     2.409871   \n",
       "4                               301.983950    15.302549     3.992740   \n",
       "\n",
       "   5__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "0                               324.515880   \n",
       "1                                84.725142   \n",
       "2                                76.240782   \n",
       "3                               112.547521   \n",
       "4                               172.564336   \n",
       "\n",
       "   5__fft_coefficient__coeff_1__attr_\"abs\"  5__kurtosis  5__skewness  \\\n",
       "0                               251.116361     0.072799     0.804929   \n",
       "1                                27.013154    -0.348744     0.132025   \n",
       "2                                73.533783     0.063837    -0.175525   \n",
       "3                               123.413344     1.793238     0.953107   \n",
       "4                               289.757599    12.507289     3.513977   \n",
       "\n",
       "   flux__length  flux__longest_strike_above_mean  \\\n",
       "0         330.0                             15.0   \n",
       "1         330.0                             10.0   \n",
       "2         352.0                             15.0   \n",
       "3         330.0                             10.0   \n",
       "4         352.0                             24.0   \n",
       "\n",
       "   flux__longest_strike_below_mean  flux__mean_abs_change  flux__mean_change  \\\n",
       "0                             32.0               4.426774           0.017885   \n",
       "1                             15.0               3.055953          -0.025935   \n",
       "2                             15.0               3.247664           0.024840   \n",
       "3                             12.0               3.299337          -0.001116   \n",
       "4                             52.0               5.054175          -0.026764   \n",
       "\n",
       "   flux_by_flux_ratio_sq__longest_strike_above_mean  \\\n",
       "0                                               9.0   \n",
       "1                                               4.0   \n",
       "2                                               4.0   \n",
       "3                                               4.0   \n",
       "4                                              10.0   \n",
       "\n",
       "   flux_by_flux_ratio_sq__longest_strike_below_mean  mjd__mean_abs_change  \\\n",
       "0                                             222.0              2.350061   \n",
       "1                                              85.0              9.335100   \n",
       "2                                              75.0              0.690700   \n",
       "3                                              98.0              0.602500   \n",
       "4                                             203.0              1.138964   \n",
       "\n",
       "   mjd__mean_change  mjd_diff_det  hostgal_photoz  hostgal_photoz_err  \\\n",
       "0          2.350061      119.8531          0.3193              0.0542   \n",
       "1          9.335100       28.0053          0.6323              0.0179   \n",
       "2          0.690700        2.7628          0.8297              0.0605   \n",
       "3          0.602500        3.0125          0.6533              0.1479   \n",
       "4          1.138964       25.0572          0.4617              0.0122   \n",
       "\n",
       "   distmod  mwebv  haversine   latlon1  hostgal_photoz_certain  A0_sum_flux  \\\n",
       "0  41.1123  0.019   1.851382  3.049709                0.337084    29.002872   \n",
       "1  42.8774  0.018   1.855173  3.009107                0.643720    22.708480   \n",
       "2  43.6000  0.016   0.309914 -1.490290                0.881446     6.030863   \n",
       "3  42.9640  0.023   1.845038  3.078301                0.757434     0.776942   \n",
       "4  42.0540  0.023   0.391772 -1.993709                0.467367    65.064186   \n",
       "\n",
       "   A0_mean_flux  A0_std_detected  A1_mean_detected  A2_sum_detected  \\\n",
       "0      0.402818           0.0000           0.07690             11.0   \n",
       "1      0.315396           0.0000           0.01923              1.0   \n",
       "2      0.095728           0.0000           0.03450              2.0   \n",
       "3      0.010791           0.0000           0.00000              2.0   \n",
       "4      1.032765           0.1768           0.06900              5.0   \n",
       "\n",
       "   A4_mean_detected  A5_std_detected  A5_mean_detected  \\\n",
       "0            0.3137           0.3254           0.11770   \n",
       "1            0.0000           0.0000           0.00000   \n",
       "2            0.0000           0.0000           0.00000   \n",
       "3            0.0392           0.0000           0.00000   \n",
       "4            0.0690           0.2253           0.05264   \n",
       "\n",
       "   percent_p2_region_minus_1  A2_min_flux  A5_sum_detected  \\\n",
       "0                     0.7114    -1.778855              6.0   \n",
       "1                     0.6150    -2.655194              0.0   \n",
       "2                     0.6206    -2.790141              0.0   \n",
       "3                     0.3845    -4.254661              0.0   \n",
       "4                     0.9136    -3.601765              3.0   \n",
       "\n",
       "   __flux_percentile_ratio_mid50___5_  __flux_percentile_ratio_mid65___2_  \\\n",
       "0                            0.005627                            0.528087   \n",
       "1                            0.014907                            0.278418   \n",
       "2                            0.000377                            0.323662   \n",
       "3                            0.013710                            0.402374   \n",
       "4                            0.005099                            0.597406   \n",
       "\n",
       "   __median_absolute_deviation___2_  __qso_log_chi2_qsonu___0_  \\\n",
       "0                          1.384690                  -0.007597   \n",
       "1                          0.813900                   0.266805   \n",
       "2                          0.845784                   0.335972   \n",
       "3                          0.906290                   0.039595   \n",
       "4                          1.026020                   0.976849   \n",
       "\n",
       "   __stetson_k___1_  __freq1_signif___2_  __stetson_k___2_  \\\n",
       "0          0.610190              4.81984          0.681284   \n",
       "1          0.640702              3.31422          0.665239   \n",
       "2          0.798601              3.18365          0.696152   \n",
       "3          0.918965              3.55265          0.934093   \n",
       "4          0.393851              3.96443          0.359669   \n",
       "\n",
       "   __freq3_amplitude1___1_  __median_absolute_deviation___2_.1  \\\n",
       "0                 1.783390                            1.384690   \n",
       "1                 1.072370                            0.813900   \n",
       "2                 1.038100                            0.845784   \n",
       "3                 0.445288                            0.906290   \n",
       "4                 4.311070                            1.026020   \n",
       "\n",
       "   __percent_close_to_median___2_  __freq_varrat___5_  __freq_varrat___4_  \\\n",
       "0                        0.711538            0.255030            0.090550   \n",
       "1                        0.711538            0.623417            0.258451   \n",
       "2                        0.775862            0.669229            0.427519   \n",
       "3                        0.538462            0.709745            0.405740   \n",
       "4                        0.931034            0.813533            0.614606   \n",
       "\n",
       "   __qso_log_chi2_qsonu___3_  __qso_log_chi2_qsonu___1_  \\\n",
       "0                   3.499440                   2.788160   \n",
       "1                   0.848093                   1.339580   \n",
       "2                   0.684981                   1.213800   \n",
       "3                   1.512120                   0.154437   \n",
       "4                   5.150450                   4.771000   \n",
       "\n",
       "   __qso_log_chi2_qsonu___5_  __std___4_  __freq_varrat___3_  \\\n",
       "0                   1.510790    13.24940            0.124063   \n",
       "1                  -0.158567     3.44694            0.306123   \n",
       "2                   0.066402     3.80032            0.493005   \n",
       "3                   0.664241     4.61269            0.415919   \n",
       "4                   2.482000    22.24760            0.534743   \n",
       "\n",
       "   __amplitude___2_  outlierScore  hipd  lipd  highEnergy_transitory_1.0_TF  \\\n",
       "0          13.15420           1.0   1.0   1.0                             1   \n",
       "1           8.56024           0.0   1.0   1.0                             0   \n",
       "2           7.88173           0.0   1.0   1.0                             0   \n",
       "3           4.82121           0.0   1.0   1.0                             0   \n",
       "4          64.03870           0.0   1.0   1.0                             0   \n",
       "\n",
       "   highEnergy_transitory_1.5_TF  lowEnergy_transitory_1.0_TF  \\\n",
       "0                             1                            1   \n",
       "1                             0                            0   \n",
       "2                             0                            0   \n",
       "3                             0                            0   \n",
       "4                             0                            0   \n",
       "\n",
       "   lowEnergy_transitory_1.5_TF  A1_minus_3_sigma  A5_max_median_diff_flux  \\\n",
       "0                            0        -13.032169                33.023636   \n",
       "1                            0         -8.039201                13.705170   \n",
       "2                            0         -7.832717                14.683493   \n",
       "3                            0         -5.651117                26.624370   \n",
       "4                            0        -38.006542                91.693475   \n",
       "\n",
       "   A5_minus_3_sigma  A5_max_mean_diff_flux  diff_A5_A4_max_min_flux  \\\n",
       "0        -29.949324              30.439033                 1.203725   \n",
       "1        -14.805055              13.178150                10.362185   \n",
       "2        -19.129198              15.423722                 8.578852   \n",
       "3        -20.850721              25.854324                15.809828   \n",
       "4        -58.217627              87.733245               -11.865965   \n",
       "\n",
       "   diff_A2_A1_max_min_flux  diff_A3_A2_median_min_flux  \\\n",
       "0                -1.244573                    3.146350   \n",
       "1                -1.201913                    1.962808   \n",
       "2                 0.193475                    2.646153   \n",
       "3                -0.666392                    1.807202   \n",
       "4                53.806883                    0.909528   \n",
       "\n",
       "   diff_A5_A4_max_median_flux  diff_A4_A3_max_median_flux  \\\n",
       "0                   -9.275401                    3.413229   \n",
       "1                    6.524891                   -2.977615   \n",
       "2                    3.214523                    1.212961   \n",
       "3                    6.525865                    8.105404   \n",
       "4                  -18.816458                   -1.051308   \n",
       "\n",
       "   diff_A2_A0_median_min_flux  diff_A4_A3_max_mean_flux  \\\n",
       "0                   -1.906972                  0.572977   \n",
       "1                   -1.396471                 -1.799531   \n",
       "2                   -4.544296                  0.695796   \n",
       "3                    0.318818                  7.272845   \n",
       "4                   -3.743664                 -0.873642   \n",
       "\n",
       "   diff_A5_A2_max_mean_flux  diff_A5_A3_max_mean_flux  \\\n",
       "0                  9.075555                 -3.747502   \n",
       "1                 -0.690726                  3.786349   \n",
       "2                  3.460874                  4.996827   \n",
       "3                 21.129028                 14.127387   \n",
       "4                -29.398564                -18.151450   \n",
       "\n",
       "   diff_A4_A0_median_mean_flux  diff_A5_A4_max_mean_flux  \\\n",
       "0                    -7.224033                 -4.320479   \n",
       "1                     0.761845                  5.585880   \n",
       "2                    -0.503209                  4.301032   \n",
       "3                    -0.958548                  6.854541   \n",
       "4                    -5.048370                -17.277809   \n",
       "\n",
       "   diff_A2_A1_max_median_flux  diff_A5_A2_max_median_flux  \\\n",
       "0                    1.245629                    8.963043   \n",
       "1                    0.557757                   -0.730177   \n",
       "2                    1.040025                    2.092920   \n",
       "3                   -0.398379                   21.839384   \n",
       "4                   54.582170                  -32.303375   \n",
       "\n",
       "   diff_A5_A4_median_min_flux  diff_A4_A0_median_min_flux  \\\n",
       "0                   10.479126                    1.824685   \n",
       "1                    3.837294                    4.357531   \n",
       "2                    5.364329                    3.199021   \n",
       "3                    9.283963                    0.246119   \n",
       "4                    6.950493                   -2.467890   \n",
       "\n",
       "   diff_A4_A1_max_median_flux  diff_A4_A2_max_median_flux  \\\n",
       "0                   19.484073                   18.238444   \n",
       "1                   -6.697311                   -7.255068   \n",
       "2                   -0.081578                   -1.121603   \n",
       "3                   14.915140                   15.313519   \n",
       "4                   41.095253                  -13.486917   \n",
       "\n",
       "   diff_A5_A4_minus_1_sigma  diff_A5_A3_median_min_flux  \\\n",
       "0                 -0.365809                   11.064433   \n",
       "1                 -1.984715                    7.628487   \n",
       "2                 -2.637786                   10.461493   \n",
       "3                 -3.093347                    7.404063   \n",
       "4                 -1.458527                    7.316738   \n",
       "\n",
       "   diff_A5_A3_max_median_flux  diff_A3_A1_minus_1_sigma  \\\n",
       "0                   -5.862173                 -1.251189   \n",
       "1                    3.547276                  0.777005   \n",
       "2                    4.427483                 -0.369331   \n",
       "3                   14.631269                 -0.262982   \n",
       "4                  -19.867766                 -6.951712   \n",
       "\n",
       "   diff_A3_A0_median_min_flux  diff_A3_A0_plus_1_sigma  \\\n",
       "0                    1.239378                13.511033   \n",
       "1                    0.566338                 1.195116   \n",
       "2                   -1.898143                 0.430738   \n",
       "3                    2.126019                 2.369858   \n",
       "4                   -2.834136                24.852925   \n",
       "\n",
       "   diff_A1_A0_median_min_flux  diff_A4_A2_mean_min_flux  \\\n",
       "0                    0.583230                  8.574067   \n",
       "1                    0.363199                  4.775541   \n",
       "2                   -3.697746                  7.461872   \n",
       "3                    0.586831                  0.966333   \n",
       "4                   -2.968377                 -0.090388   \n",
       "\n",
       "   diff_A5_A1_plus_1_sigma  diff_A4_A1_median_mean_flux  \\\n",
       "0                11.763127                    -6.359525   \n",
       "1                 4.119388                     0.804491   \n",
       "2                 4.322237                     0.368021   \n",
       "3                 8.069753                    -1.239923   \n",
       "4                 6.114955                    -2.169479   \n",
       "\n",
       "   diff_A3_A2_max_median_flux  diff_A5_A1_median_mean_flux  \\\n",
       "0                   14.825216                    -1.404603   \n",
       "1                   -4.277454                    -0.134520   \n",
       "2                   -2.334563                     1.454529   \n",
       "3                    7.208114                    -0.911246   \n",
       "4                  -12.435609                    -0.630830   \n",
       "\n",
       "   div_A4_A2_median_min_flux  div_A5_A2_median_min_flux  \\\n",
       "0                   2.660059                   7.321785   \n",
       "1                   3.142918                   4.572011   \n",
       "2                   3.440466                   5.131144   \n",
       "3                   0.985034                   2.896326   \n",
       "4                   1.312649                   3.015982   \n",
       "\n",
       "   div_A5_A2_minus_1_sigma  div_A5_A4_median_mean_flux  \\\n",
       "0                 1.927288                    0.342807   \n",
       "1                 1.764152                   -1.279205   \n",
       "2                 3.490031                   -2.137666   \n",
       "3                 4.842085                    0.700856   \n",
       "4                 0.895981                    0.720189   \n",
       "\n",
       "   div_A3_A0_plus_1_sigma  div_A4_A1_minus_1_sigma  \n",
       "0                6.483993                 1.698851  \n",
       "1                1.444674                 0.735858  \n",
       "2                1.150274                 1.425184  \n",
       "3                2.143172                 1.246370  \n",
       "4                5.839107                 1.540600  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metadata_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test = test_metadata_kaggle.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp_test['object_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 137) (3492890, 137)\n"
     ]
    }
   ],
   "source": [
    "print(temp.shape,temp_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(temp.columns) == list(temp_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "CPU times: user 3h 24min 12s, sys: 43.7 s, total: 3h 24min 55s\n",
      "Wall time: 30min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pred0 = pd.DataFrame()\n",
    "test_pred1 = pd.DataFrame()\n",
    "test_pred2 = pd.DataFrame()\n",
    "test_pred3 = pd.DataFrame()\n",
    "test_pred4 = pd.DataFrame()\n",
    "\n",
    "list_of_df = [test_pred0,test_pred1,test_pred2,test_pred3,test_pred4]\n",
    "\n",
    "for num,c in enumerate(clfs):\n",
    "    print(num)\n",
    "    for k in range(0,len(temp_test),500000):\n",
    "        test_pred = pd.DataFrame(c.predict_proba(temp_test[ k:k+500000] ))\n",
    "        list_of_df[num] = pd.concat([list_of_df[num],test_pred],axis=0)\n",
    "        del test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred2 = pd.DataFrame()\n",
    "test_pred2 = (list_of_df[0] + list_of_df[1] + list_of_df[2] + list_of_df[3] + list_of_df[4])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3492890, 14)\n"
     ]
    }
   ],
   "source": [
    "print(test_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_pred2 = pd.DataFrame(np.random.rand(10,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.776456</td>\n",
       "      <td>0.065814</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.087279</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.063397</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.001427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.153913</td>\n",
       "      <td>0.036415</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.035604</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.021963</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>0.727573</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.013090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.016769</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.143189</td>\n",
       "      <td>0.098407</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.036879</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.069276</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>0.579812</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.042610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.048005</td>\n",
       "      <td>0.025962</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.094537</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.578677</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.180624</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.061447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.056203</td>\n",
       "      <td>0.260264</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.667468</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000159  0.001374  0.000114  0.776456  0.065814  0.000287  0.087279   \n",
       "1  0.000095  0.008413  0.000097  0.153913  0.036415  0.000203  0.035604   \n",
       "2  0.000197  0.016769  0.000211  0.143189  0.098407  0.000419  0.036879   \n",
       "3  0.000228  0.002427  0.000429  0.048005  0.025962  0.000429  0.094537   \n",
       "4  0.000058  0.001247  0.000061  0.056203  0.260264  0.000113  0.009146   \n",
       "\n",
       "         7         8         9         10        11        12        13  \n",
       "0  0.000033  0.000113  0.003116  0.000222  0.063397  0.000210  0.001427  \n",
       "1  0.000157  0.000418  0.021963  0.001872  0.727573  0.000188  0.013090  \n",
       "2  0.006017  0.000540  0.069276  0.005418  0.579812  0.000256  0.042610  \n",
       "3  0.005604  0.000403  0.578677  0.000932  0.180624  0.000294  0.061447  \n",
       "4  0.000026  0.000082  0.005087  0.000074  0.667468  0.000072  0.000098  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_columns = ['object_id','class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred2.columns = temp_columns[1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUnknown(data):\n",
    "    return ((((((data[\"mymedian\"]) + (((data[\"mymean\"]) / 2.0)))/2.0)) + (((((1.0) - (((data[\"mymax\"]) * (((data[\"mymax\"]) * (data[\"mymax\"]))))))) / 2.0)))/2.0)\n",
    "\n",
    "feats = ['class_6', 'class_15', 'class_16', 'class_42', 'class_52', 'class_53',\n",
    "         'class_62', 'class_64', 'class_65', 'class_67', 'class_88', 'class_90',\n",
    "         'class_92', 'class_95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "klm = pd.DataFrame()\n",
    "klm['mymean'] = test_pred2[feats].mean(axis=1)\n",
    "klm['mymedian'] = test_pred2[feats].median(axis=1)\n",
    "klm['mymax'] = test_pred2[feats].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred2['class_99'] = getUnknown(klm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_15</th>\n",
       "      <th>class_16</th>\n",
       "      <th>class_42</th>\n",
       "      <th>class_52</th>\n",
       "      <th>class_53</th>\n",
       "      <th>class_62</th>\n",
       "      <th>class_64</th>\n",
       "      <th>class_65</th>\n",
       "      <th>class_67</th>\n",
       "      <th>class_88</th>\n",
       "      <th>class_90</th>\n",
       "      <th>class_92</th>\n",
       "      <th>class_95</th>\n",
       "      <th>class_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>492885</th>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.387601</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.187291</td>\n",
       "      <td>0.117463</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.129291</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>0.153796</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.244821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492886</th>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.040771</td>\n",
       "      <td>0.208497</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.015844</td>\n",
       "      <td>0.429028</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.022308</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.265852</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.241115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492887</th>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.026040</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.895030</td>\n",
       "      <td>0.034262</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.030965</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.007972</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.079937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492888</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.691828</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.145591</td>\n",
       "      <td>0.004017</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.012245</td>\n",
       "      <td>0.115592</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.028841</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.176271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492889</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.096712</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.468399</td>\n",
       "      <td>0.181191</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.064480</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.028040</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.156185</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.233530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class_6  class_15  class_16  class_42  class_52  class_53  class_62  \\\n",
       "492885  0.000216  0.387601  0.000385  0.187291  0.117463  0.000445  0.129291   \n",
       "492886  0.000220  0.008853  0.000190  0.040771  0.208497  0.000268  0.015844   \n",
       "492887  0.000755  0.026040  0.000174  0.895030  0.034262  0.000385  0.030965   \n",
       "492888  0.000067  0.691828  0.000094  0.145591  0.004017  0.000128  0.012245   \n",
       "492889  0.000262  0.096712  0.000311  0.468399  0.181191  0.000568  0.064480   \n",
       "\n",
       "        class_64  class_65  class_67  class_88  class_90  class_92  class_95  \\\n",
       "492885  0.000314  0.001470  0.018458  0.002131  0.153796  0.000287  0.000851   \n",
       "492886  0.429028  0.000393  0.022308  0.000971  0.265852  0.000232  0.006573   \n",
       "492887  0.000069  0.000187  0.001288  0.000457  0.007972  0.000244  0.002171   \n",
       "492888  0.115592  0.000447  0.000294  0.000210  0.028841  0.000099  0.000548   \n",
       "492889  0.000126  0.000655  0.028040  0.001414  0.156185  0.000732  0.000925   \n",
       "\n",
       "        class_99  \n",
       "492885  0.244821  \n",
       "492886  0.241115  \n",
       "492887  0.079937  \n",
       "492888  0.176271  \n",
       "492889  0.233530  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred2 = test_pred2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3492890, 15) (3492890,)\n"
     ]
    }
   ],
   "source": [
    "print(test_pred2.shape,test_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3492885    130787966\n",
       "3492886    130787971\n",
       "3492887    130787974\n",
       "3492888    130788053\n",
       "3492889    130788054\n",
       "Name: object_id, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_id.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id.index == test_pred2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 89.6 ms, sys: 142 ms, total: 231 ms\n",
      "Wall time: 230 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pred = pd.concat([test_id,test_pred2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred[temp_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_15</th>\n",
       "      <th>class_16</th>\n",
       "      <th>class_42</th>\n",
       "      <th>class_52</th>\n",
       "      <th>class_53</th>\n",
       "      <th>class_62</th>\n",
       "      <th>class_64</th>\n",
       "      <th>class_65</th>\n",
       "      <th>class_67</th>\n",
       "      <th>class_88</th>\n",
       "      <th>class_90</th>\n",
       "      <th>class_92</th>\n",
       "      <th>class_95</th>\n",
       "      <th>class_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.776456</td>\n",
       "      <td>0.065814</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.087279</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.063397</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.142108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.153913</td>\n",
       "      <td>0.036415</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.035604</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.021963</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>0.727573</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.163927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.016769</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.143189</td>\n",
       "      <td>0.098407</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.036879</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.069276</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>0.579812</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.042610</td>\n",
       "      <td>0.213046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.048005</td>\n",
       "      <td>0.025962</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.094537</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.578677</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.180624</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.061447</td>\n",
       "      <td>0.211487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.056203</td>\n",
       "      <td>0.260264</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.667468</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.184613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id   class_6  class_15  class_16  class_42  class_52  class_53  \\\n",
       "0         13  0.000159  0.001374  0.000114  0.776456  0.065814  0.000287   \n",
       "1         14  0.000095  0.008413  0.000097  0.153913  0.036415  0.000203   \n",
       "2         17  0.000197  0.016769  0.000211  0.143189  0.098407  0.000419   \n",
       "3         23  0.000228  0.002427  0.000429  0.048005  0.025962  0.000429   \n",
       "4         34  0.000058  0.001247  0.000061  0.056203  0.260264  0.000113   \n",
       "\n",
       "   class_62  class_64  class_65  class_67  class_88  class_90  class_92  \\\n",
       "0  0.087279  0.000033  0.000113  0.003116  0.000222  0.063397  0.000210   \n",
       "1  0.035604  0.000157  0.000418  0.021963  0.001872  0.727573  0.000188   \n",
       "2  0.036879  0.006017  0.000540  0.069276  0.005418  0.579812  0.000256   \n",
       "3  0.094537  0.005604  0.000403  0.578677  0.000932  0.180624  0.000294   \n",
       "4  0.009146  0.000026  0.000082  0.005087  0.000074  0.667468  0.000072   \n",
       "\n",
       "   class_95  class_99  \n",
       "0  0.001427  0.142108  \n",
       "1  0.013090  0.163927  \n",
       "2  0.042610  0.213046  \n",
       "3  0.061447  0.211487  \n",
       "4  0.000098  0.184613  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3492890, 16)\n"
     ]
    }
   ],
   "source": [
    "print(test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 1.36 s, total: 1min 42s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pred.to_csv('test_pred_37.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions submit -c PLAsTiCC-2018 -f test_pred_27.csv -m \"Message\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
